.. _references:

Paper References
================

* [1] 2020 Nature Nanotechnology,
  `Memory devices and applications for in-memory computing`_

* [2] 2020 Nature Communications.
  `Accurate deep neural network inference using computational phase-change memory`_

* [3] 2020 Frontiers in Neuroscience,
  `Acceleration of deep neural network training with resistive cross-point devices: Design considerations`_

* [4] 2020 Frontiers in Neuroscience,
  `Mixed-precision deep learning based on computational memory`_

* [5] 2018 Nature,
  `Equivalent-accuracy accelerated neural-network training using analogue memory`_

* [6] 2018 Nature Communications, 
  `Signal and noise extraction from analog memory elements for neuromorphic computing`_

* [7] 2019 IEEE Symposium on VLSI Technologies,
  `Capacitor-based Cross-point Array for Analog Neural Network with Record Symmetry and Linearity`_  

* [8] 2018 International Electron Devices Meeting (IEDM),
  `ECRAM as Scalable Synaptic Cell for High-Speed, Low-Power Neuromorphic Computing`_  

* [9] 2016 Frontiers in Neuroscience,
  `Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices: Design Considerations`_  

* [10] 2020 Frontiers in Neuroscience,
  `Algorithm for Training Neural Networks on Resistive Device Arrays`_

* [11] 2023 APL Machine Lerning,
  `Using the IBM analog in-memory hardware acceleration kit for neural network training and inference`_

* [12] 2023 Nature Communications,
  `Hardware-aware training for large-scale and diverse deep learning inference workloads using in-memory computing-based accelerators`_

* [13] 2023 Nature Electronics,
  `A 64-core mixed-signal in-memory compute chip based on phase-change memory for deep neural network inference`_

* [14] 2023 Nature,
  `An analog-AI chip for energy-efficient speech recognition and transcription`_


.. _`Memory devices and applications for in-memory computing`: https://www.nature.com/articles/s41565-020-0655-z
.. _`Accurate deep neural network inference using computational phase-change memory`: https://www.nature.com/articles/s41467-020-16108-9
.. _`Acceleration of deep neural network training with resistive cross-point devices: Design considerations`: https://www.frontiersin.org/articles/10.3389/fnins.2016.00333/full
.. _`Mixed-precision deep learning based on computational memory`: https://www.frontiersin.org/articles/10.3389/fnins.2020.00406/full
.. _`Equivalent-accuracy accelerated neural-network training using analogue memory`: https://www.nature.com/articles/s41586-018-0180-5
.. _`Signal and noise extraction from analog memory elements for neuromorphic computing`: https://www.nature.com/articles/s41467-018-04485-1
.. _`Capacitor-based Cross-point Array for Analog Neural Network with Record Symmetry and Linearity`: https://ieeexplore.ieee.org/document/8510648
.. _`ECRAM as Scalable Synaptic Cell for High-Speed, Low-Power Neuromorphic Computing`: https://ieeexplore.ieee.org/document/8614551
.. _`Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices- Design Considerations`: https://www.frontiersin.org/articles/10.3389/fnins.2016.00333/full
.. _`Algorithm for Training Neural Networks on Resistive Device Arrays`: https://www.frontiersin.org/articles/10.3389/fnins.2020.00103/full
.. _`Using the IBM analog in-memory hardware acceleration kit for neural network training and inference`: https://pubs.aip.org/aip/aml/article/1/4/041102/2923573
.. _`Hardware-aware training for large-scale and diverse deep learning inference workloads using in-memory computing-based accelerators`: https://www.nature.com/articles/s41467-023-40770-4
.. _`A 64-core mixed-signal in-memory compute chip based on phase-change memory for deep neural network inference`: https://www.nature.com/articles/s41928-023-01010-1
.. _`An analog-AI chip for energy-efficient speech recognition and transcription`: https://www.nature.com/articles/s41586-023-06337-5




