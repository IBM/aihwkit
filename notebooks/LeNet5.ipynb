{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Analog AI\n",
    "\n",
    "In-memory computing hardware increases the speed and energy-efficiency needed for the next steps in AI. Analog AI delivers radical performance improvements by combining compute and memory in a single device, eliminating the von Neumann bottleneck. \n",
    "Based on von Neumann architecture, conventional computers perform calculations by repeatedly transferring data between the memory and processor. These trips require time and energy, negatively impacting performance. This is known as the von Neumann bottleneck. \n",
    "\n",
    "<img src=\"processing-unit-and-computional-memory.png\" style=\"width:30%; height:30%\" caption=\"Processing unit and conventional memory\"/> \n",
    "<img src=\"processing-unit-and-conventional-memory.png\" style=\"width:30%; height:30%\"/> \n",
    "\n",
    "By leveraging the physical properties of in-memory computing devices (example: Phase Change Memory or PCM), computation happens at the same place where the data is stored, drastically reducing energy consumption. Because there is no movement of data, tasks can be performed in a fraction of the time and with much less energy. This is different from a conventional computer, where the data is transferred from the DRAM memory to the CPU every time a computation is done. For example, moving 64 bits of data from DRAM to CPU consumes 1-2nJ, which is 10,000–2,000,000 times more energy than is dissipated in a PCM device performing a multiplication operation (1-100fJ). Also, PCM does not consume power when the devices are inactive, and the data will be retained for up to 10 years even when the power supply is turned off. \n",
    "\n",
    "## The physics behind PCM\n",
    "\n",
    " With PCM, when an electrical pulse is applied to the material, it changes the conductance of the device by switching the material between amorphous and crystalline phases. A low electrical pulse will make the PCM device more crystalline (less resistance). A high electrical pulse will make the device more amorphous (more resistance). Therefore, instead of recording a 0 or 1 like in the digital world, it records the states as a continuum of values between the two–the analog world.\n",
    "\n",
    "PCM devices have the ability to store synaptic weights in their analog conductance state. When PCM devices are arranged in a crossbar configuration, it allows to perform an analog matrix-vector multiplication in a single time step, exploiting the advantages of multi-level storage capability and Kirchhoff’s circuits laws. The figure below shows how PCM devices are arranged in a crossbar configuration. \n",
    "<center><img src=\"pcm-array.png\" style=\"width:30%; height:30%\"/></center> \n",
    "\n",
    "This crossbar configuration is also referred to as an Analog tile. The PCM devices at each crossbard crosspoint are also referred to as Resitive Processing Units or RPU units as shown in the figure below:\n",
    "\n",
    "<center><img src=\"pcm_rpu_unit.png\" style=\"width:30%; height:30%\"/></center> \n",
    "\n",
    "Besides PCM, other devices or materials can be used as resistive units or RPUs in the crossbar configuration. Examples include Resistive Randam Access memory (RRAM), Electro chemical Random-access memory (ECRAM), Magnetic RAM (MRAM), photonics, etc. \n",
    "\n",
    "\n",
    "## Analog AI and Neural Networks\n",
    "\n",
    "In deep learning inference, data propagation through multiple layers of a neural network involves a sequence of matrix multiplications, as each layer can be represented as a matrix of synaptic weights. On an Analog chip, these weights are stored in the conductance states of resistive devices such as PCM. The devices are arranged in crossbar arrays, creating an artificial neural network where all matrix multiplications are performed in-place in an analog manner. This structure allows inference to be performed using little energy with high areal density of synapses. An in-memory computing chip typically consists of multiple crossbar arrays of memory devices that communicate with each other (see figure below). A neural network layer can be implemented on (at least) one crossbar, in which the weights of that layer are stored in the charge or conductance state of the memory devices at the crosspoints.\n",
    "\n",
    "<center><img src=\"analog_Dnn.png\" style=\"width:60%; height:60%\"/></center> \n",
    "\n",
    "\n",
    "\n",
    "## IBM Analog Hardware Acceleration Kit (AIHWKIT)\n",
    "\n",
    "The IBM Analog Hardware Acceleration Kit (AIHWKIT) is an open source Python toolkit for exploring and using the capabilities of in-memory computing devices such as PCM in the context of artificial intelligence. \n",
    "The pytorch integration consists of a series of primitives and features that allow using the toolkit within PyTorch. \n",
    "The github repository can be found at: https://github.com/IBM/aihwkit\n",
    "\n",
    "To learn more about Analog AI and the harware befind it, refer to this webpage: https://analog-ai-demo.mybluemix.net/hardware\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to install the AIHHKIT and dependencies in your environment. The preferred way to install this package is by using the Python package index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aihwkit\n",
      "  Downloading aihwkit-0.4.0-cp38-cp38-macosx_10_9_x86_64.whl (10.4 MB)\n",
      "     |████████████████████████████████| 10.4 MB 256 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from aihwkit) (1.4.1)\n",
      "Collecting requests<3,>=2.25\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "     |████████████████████████████████| 62 kB 1.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13.0 in /Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from aihwkit) (3.13.0)\n",
      "Requirement already satisfied: numpy>=1.18 in /Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from aihwkit) (1.19.4)\n",
      "Requirement already satisfied: torchvision in /Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from aihwkit) (0.9.0)\n",
      "Collecting torch==1.8.1\n",
      "  Downloading torch-1.8.1-cp38-none-macosx_10_9_x86_64.whl (119.6 MB)\n",
      "     |████████████████████████████████| 119.6 MB 3.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from torch==1.8.1->aihwkit) (3.7.4.3)\n",
      "Requirement already satisfied: setuptools in /Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from protobuf<4,>=3.13.0->aihwkit) (54.1.1)\n",
      "Requirement already satisfied: six>=1.9 in /Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from protobuf<4,>=3.13.0->aihwkit) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from requests<3,>=2.25->aihwkit) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from requests<3,>=2.25->aihwkit) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from requests<3,>=2.25->aihwkit) (1.25.10)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.8-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages (from torchvision->aihwkit) (8.1.2)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.11.1-cp38-cp38-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 2.1 MB/s            \n",
      "\u001b[?25h  Downloading torchvision-0.10.1-cp38-cp38-macosx_10_9_x86_64.whl (14.3 MB)\n",
      "     |████████████████████████████████| 14.3 MB 2.6 MB/s            \n",
      "\u001b[?25h  Using cached torchvision-0.10.0-cp38-cp38-macosx_10_9_x86_64.whl (13.9 MB)\n",
      "  Downloading torchvision-0.9.1-cp38-cp38-macosx_10_9_x86_64.whl (13.2 MB)\n",
      "     |████████████████████████████████| 13.2 MB 1.3 MB/s            \n",
      "\u001b[?25hInstalling collected packages: torch, charset-normalizer, torchvision, requests, aihwkit\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.8.0\n",
      "    Uninstalling torch-1.8.0:\n",
      "      Successfully uninstalled torch-1.8.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.9.0\n",
      "    Uninstalling torchvision-0.9.0:\n",
      "      Successfully uninstalled torchvision-0.9.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "Successfully installed aihwkit-0.4.0 charset-normalizer-2.0.8 requests-2.26.0 torch-1.8.1 torchvision-0.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install aihwkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use the AIHWKIT to train a LeNet5 inspired analog network, as studied in the paper: https://www.frontiersin.org/articles/10.3389/fnins.2017.00538/full\n",
    "\n",
    "<img src=\"LeNet5_animation.png\" style=\"width:40%; height:40%\"/> \n",
    "\n",
    "The network will be trained using the MNIST dataset, a collection of images representing the digits 0 to 9. The architecture of the LeNet5 network is shown below:\n",
    "\n",
    "<img src=\"LeNet.png\" style=\"width:60%; height:60%\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the library was installed correctly, you can use the following snippet for creating an analog layer and predicting the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2824, -0.0941],\n",
       "        [ 0.2353, -0.1412]], grad_fn=<AnalogFunctionBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "from aihwkit.nn import AnalogLinear\n",
    "\n",
    "model = AnalogLinear(2, 2)\n",
    "model(Tensor([[0.1, 0.2], [0.3, 0.4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the package is installed and running, we can start working on creating the LeNet5 network.\n",
    "\n",
    "AIHWKIT offers different Analog layers that can be used to build a network, including AnalogLinear and AnalogConv2d which will be the main layers used to build the present network. \n",
    "In addition to the standard input that are expected by the PyTorch layers (in_channels, out_channels, etc.) the analog layers also expect a rpu_config input which defines various settings of the RPU tile. \n",
    "\n",
    "Through the rpu_config parameter the user can specify many of the hardware specs such as: device used in the cross-point array, bit used by the ADC/DAC converters, noise values and many other. Additional details on the RPU configuration can be found at https://aihwkit.readthedocs.io/en/latest/using_simulator.html#rpu-configurations\n",
    "For this particular case we will define a RPU which uses RRAM devices in the cross-point array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rpu_config():\n",
    "\n",
    "    from aihwkit.simulator.presets import ReRamSBPreset\n",
    "\n",
    "    rpu_config=ReRamSBPreset()\n",
    "\n",
    "    return rpu_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this rpu_config as input of the network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Tanh, MaxPool2d, LogSoftmax, Flatten\n",
    "from aihwkit.nn import AnalogConv2d, AnalogLinear, AnalogSequential\n",
    "\n",
    "def create_analog_network(rpu_config):\n",
    "    \n",
    "    channel = [16, 32, 512, 128]\n",
    "    model = AnalogSequential(\n",
    "        AnalogConv2d(in_channels=1, out_channels=channel[0], kernel_size=5, stride=1,\n",
    "                        rpu_config=rpu_config),\n",
    "        Tanh(),\n",
    "        MaxPool2d(kernel_size=2),\n",
    "        AnalogConv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=5, stride=1,\n",
    "                        rpu_config=rpu_config),\n",
    "        Tanh(),\n",
    "        MaxPool2d(kernel_size=2),\n",
    "        Tanh(),\n",
    "        Flatten(),\n",
    "        AnalogLinear(in_features=channel[2], out_features=channel[3], rpu_config=rpu_config),\n",
    "        Tanh(),\n",
    "        AnalogLinear(in_features=channel[3], out_features=10, rpu_config=rpu_config),\n",
    "        LogSoftmax(dim=1)\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the cross entropy to calculate the loss and the Stochastic Gradient Descent (SGD) as optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "\n",
    "from aihwkit.optim import AnalogSGD\n",
    "\n",
    "def create_analog_optimizer(model):\n",
    "    \"\"\"Create the analog-aware optimizer.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): model to be trained\n",
    "\n",
    "    Returns:\n",
    "        Optimizer: created analog optimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = AnalogSGD(model.parameters(), lr=0.01) # we will use a learning rate of 0.01 as in the paper\n",
    "    optimizer.regroup_param_groups(model)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write the train function which will optimize the network over the MNIST train dataset. The train_step function will take as input the images to train on, the model to train and the criterion and optimizer to train with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import device\n",
    "\n",
    "DEVICE = device('cpu') # we define for now the device as cpu\n",
    "\n",
    "def train_step(train_data, model, criterion, optimizer):\n",
    "    \"\"\"Train network.\n",
    "\n",
    "    Args:\n",
    "        train_data (DataLoader): Validation set to perform the evaluation\n",
    "        model (nn.Module): Trained model to be evaluated\n",
    "        criterion (nn.CrossEntropyLoss): criterion to compute loss\n",
    "        optimizer (Optimizer): analog model optimizer\n",
    "\n",
    "    Returns:\n",
    "        train_dataset_loss: epoch loss of the train dataset\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for images, labels in train_data:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Add training Tensor to the model (input).\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Run training (backward propagation).\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize weights.\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "    train_dataset_loss = total_loss / len(train_data.dataset)\n",
    "\n",
    "    return train_dataset_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can be quite time consuming it is nice to see the evolution of the training process by testing the model capabilities on a set of images that it has not seen before (test dataset). So we write a test_step function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(validation_data, model, criterion):\n",
    "    \"\"\"Test trained network\n",
    "\n",
    "    Args:\n",
    "        validation_data (DataLoader): Validation set to perform the evaluation\n",
    "        model (nn.Module): Trained model to be evaluated\n",
    "        criterion (nn.CrossEntropyLoss): criterion to compute loss\n",
    "\n",
    "    Returns: \n",
    "        test_dataset_loss: epoch loss of the train_dataset\n",
    "        test_dataset_error: error of the test dataset\n",
    "        test_dataset_accuracy: accuracy of the test dataset\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    predicted_ok = 0\n",
    "    total_images = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for images, labels in validation_data:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        pred = model(images)\n",
    "        loss = criterion(pred, labels)\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        total_images += labels.size(0)\n",
    "        predicted_ok += (predicted == labels).sum().item()\n",
    "        test_dataset_accuracy = predicted_ok/total_images*100\n",
    "        test_dataset_error = (1-predicted_ok/total_images)*100\n",
    "\n",
    "    test_dataset_loss = total_loss / len(validation_data.dataset)\n",
    "\n",
    "    return test_dataset_loss, test_dataset_error, test_dataset_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reach satisfactory accuracy levels, the train_step will have to be repeated mulitple time so we will implement a loop over a certain number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, criterion, optimizer, train_data, validation_data, epochs=10, print_every=1):\n",
    "    \"\"\"Training loop.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model to be evaluated\n",
    "        criterion (nn.CrossEntropyLoss): criterion to compute loss\n",
    "        optimizer (Optimizer): analog model optimizer\n",
    "        train_data (DataLoader): Validation set to perform the evaluation\n",
    "        validation_data (DataLoader): Validation set to perform the evaluation\n",
    "        epochs (int): global parameter to define epochs number\n",
    "        print_every (int): defines how many times to print training progress\n",
    "\n",
    "    Returns:\n",
    "        nn.Module, Optimizer, Tuple: model, optimizer, and a tuple of\n",
    "            lists of train losses, validation losses, and test error\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    test_error = []\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(0, epochs):\n",
    "        # Train_step\n",
    "        train_loss = train_step(train_data, model, criterion, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            # Validate_step\n",
    "            with torch.no_grad():\n",
    "                valid_loss, error, accuracy = test_step(validation_data, model, criterion)\n",
    "                valid_losses.append(valid_loss)\n",
    "                test_error.append(error)\n",
    "\n",
    "            print(f'Epoch: {epoch} --- '\n",
    "                  f'Train loss: {train_loss:.4f}\\t'\n",
    "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
    "                  f'Test error: {error:.2f}%\\t'\n",
    "                  f'Test accuracy: {accuracy:.2f}%\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now download the MNIST dataset and prepare the images for the training and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "PATH_DATASET = os.path.join('data', 'DATASET')\n",
    "os.makedirs(PATH_DATASET, exist_ok=True)\n",
    "\n",
    "def load_images():\n",
    "    \"\"\"Load images for train from torchvision datasets.\"\"\"\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_set = datasets.MNIST(PATH_DATASET, download=True, train=True, transform=transform)\n",
    "    test_set = datasets.MNIST(PATH_DATASET, download=True, train=False, transform=transform)\n",
    "    train_data = torch.utils.data.DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "    test_data = torch.utils.data.DataLoader(test_set, batch_size=8, shuffle=False)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put together all the code above to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/DATASET/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:08, 1227605.09it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/DATASET/MNIST/raw/train-images-idx3-ubyte.gz to data/DATASET/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 2844803.96it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/DATASET/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/DATASET/MNIST/raw/train-labels-idx1-ubyte.gz to data/DATASET/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/DATASET/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1649664it [00:01, 1573844.28it/s]                             \n",
      "5120it [00:00, 4918652.42it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/DATASET/MNIST/raw/t10k-images-idx3-ubyte.gz to data/DATASET/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/DATASET/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/DATASET/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/DATASET/MNIST/raw\n",
      "\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaoutar/.pyenv/versions/3.8.3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Epoch: 0 --- Train loss: 2.9993\tValid loss: 2.7943\tTest error: 90.71%\tTest accuracy: 9.29%\t\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#load the dataset\n",
    "train_data, test_data = load_images()\n",
    "\n",
    "#create the rpu_config\n",
    "rpu_config = create_rpu_config()\n",
    "\n",
    "#create the model\n",
    "model = create_analog_network(rpu_config)\n",
    "\n",
    "#define the analog optimizer\n",
    "optimizer = create_analog_optimizer(model)\n",
    "\n",
    "training_loop(model, criterion, optimizer, train_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8afadb82c8c635d284d204a78cd7f3b56094702ee8f92f25084bfbbc5b27362b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('vs': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
