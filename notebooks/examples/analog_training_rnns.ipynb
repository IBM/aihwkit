{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25qk591U8huF"
   },
   "source": [
    "# Writing In the Style of Arthur Conan Doyle\n",
    "In this example, we'll create an model that can generate text and write in the style of Sir Arthur Conan Doyle. We will go through the steps of training an RNN model and evaluate how well Analog training performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DuMM-3JQy0Xj",
    "outputId": "9062ea4d-ed3a-44aa-bc9b-86362863c324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5FHitt2mzgou",
    "outputId": "e6313683-0f82-4201-894d-24d8a9011e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.13\n",
      "--2022-05-27 13:30:44--  https://aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud/aihwkit-0.6.0.cuda111-cp37-cp37m-manylinux2014_x86_64.whl\n",
      "Resolving aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud (aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud)... 169.63.118.98\n",
      "Connecting to aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud (aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud)|169.63.118.98|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 346965225 (331M) [application/octet-stream]\n",
      "Saving to: ‘aihwkit-0.6.0.cuda111-cp37-cp37m-manylinux2014_x86_64.whl’\n",
      "\n",
      "aihwkit-0.6.0.cuda1 100%[===================>] 330.89M  77.2MB/s    in 4.6s    \n",
      "\n",
      "2022-05-27 13:30:49 (72.4 MB/s) - ‘aihwkit-0.6.0.cuda111-cp37-cp37m-manylinux2014_x86_64.whl’ saved [346965225/346965225]\n",
      "\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Processing ./aihwkit-0.6.0.cuda111-cp37-cp37m-manylinux2014_x86_64.whl\n",
      "Collecting torchvision==0.11.1+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.11.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (24.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.5 MB 1.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from aihwkit==0.6.0.cuda111) (1.21.6)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /usr/local/lib/python3.7/dist-packages (from aihwkit==0.6.0.cuda111) (3.17.3)\n",
      "Collecting torch==1.10.0+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torch-1.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2137.6 MB)\n",
      "\u001b[K     |████████████▌                   | 834.1 MB 1.3 MB/s eta 0:16:36tcmalloc: large alloc 1147494400 bytes == 0x3b5e000 @  0x7fb84e21f615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
      "\u001b[K     |███████████████▉                | 1055.7 MB 1.2 MB/s eta 0:14:55tcmalloc: large alloc 1434370048 bytes == 0x481b4000 @  0x7fb84e21f615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
      "\u001b[K     |████████████████████            | 1336.2 MB 1.2 MB/s eta 0:10:43tcmalloc: large alloc 1792966656 bytes == 0x9d9a0000 @  0x7fb84e21f615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
      "\u001b[K     |█████████████████████████▎      | 1691.1 MB 1.4 MB/s eta 0:05:31tcmalloc: large alloc 2241208320 bytes == 0x3b5e000 @  0x7fb84e21f615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
      "\u001b[K     |████████████████████████████████| 2137.6 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 2137645056 bytes == 0x894c0000 @  0x7fb84e21e1e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n",
      "tcmalloc: large alloc 2672058368 bytes == 0x108b5e000 @  0x7fb84e21f615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n",
      "\u001b[K     |████████████████████████████████| 2137.6 MB 421 bytes/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from aihwkit==0.6.0.cuda111) (1.4.1)\n",
      "Collecting requests<3,>=2.25\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0+cu111->aihwkit==0.6.0.cuda111) (4.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1+cu111->aihwkit==0.6.0.cuda111) (7.1.2)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.13.0->aihwkit==0.6.0.cuda111) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25->aihwkit==0.6.0.cuda111) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25->aihwkit==0.6.0.cuda111) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25->aihwkit==0.6.0.cuda111) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.25->aihwkit==0.6.0.cuda111) (2.0.12)\n",
      "Installing collected packages: torch, torchvision, requests, aihwkit\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0+cu113\n",
      "    Uninstalling torch-1.11.0+cu113:\n",
      "      Successfully uninstalled torch-1.11.0+cu113\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.12.0+cu113\n",
      "    Uninstalling torchvision-0.12.0+cu113:\n",
      "      Successfully uninstalled torchvision-0.12.0+cu113\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.0+cu111 which is incompatible.\n",
      "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.0+cu111 which is incompatible.\n",
      "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Successfully installed aihwkit-0.6.0.cuda111 requests-2.27.1 torch-1.10.0+cu111 torchvision-0.11.1+cu111\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!wget https://aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud/aihwkit-0.6.0.cuda111-cp37-cp37m-manylinux2014_x86_64.whl\n",
    "!pip install aihwkit-0.6.0.cuda111-cp37-cp37m-manylinux2014_x86_64.whl -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eelfjm0LzTIu"
   },
   "outputs": [],
   "source": [
    "#Import from aihwkit\n",
    "from aihwkit.nn import AnalogRNN, AnalogLSTMCell, AnalogGRUCell, AnalogVanillaRNNCell\n",
    "from aihwkit.optim import AnalogSGD\n",
    "from aihwkit.simulator.configs import SingleRPUConfig\n",
    "from aihwkit.simulator.configs import InferenceRPUConfig\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    WeightNoiseType, WeightClipType, WeightModifierType)\n",
    "from aihwkit.simulator.presets import GokmenVlasovPreset\n",
    "from aihwkit.inference import PCMLikeNoiseModel, GlobalDriftCompensation\n",
    "from aihwkit.nn import AnalogLinear, AnalogSequential\n",
    "\n",
    "#other imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define some hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5A-o6_JT2EGn"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 4096\n",
    "EMBED_SIZE = 512\n",
    "HIDDEN_SIZE = 1024\n",
    "DROPOUT_RATIO = 0.0\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LEN = 32\n",
    "RNN_CELL = AnalogGRUCell #type of RNN cell\n",
    "WITH_BIDIR = True\n",
    "USE_ANALOG_TRAINING = False  # or hardware-aware training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ9-iJXO8vPL"
   },
   "source": [
    "Next, we'll specify a configuration for our Resistive Processing Unit (RPU). The RPU is the actual analog memory device that stores the weights of the network. We can either train our model in analog, or train in a traditional digital approach which takes into account the non-idealities of analog devices during training. This is known as hardware-aware training and is the method that we'll use in this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HuQdUjG87AH"
   },
   "outputs": [],
   "source": [
    "if USE_ANALOG_TRAINING:\n",
    "    # Define a RPU configuration for analog training\n",
    "    rpu_config = GokmenVlasovPreset()\n",
    "\n",
    "else:\n",
    "    # Define an RPU configuration using inference/hardware-aware training tile\n",
    "    rpu_config = InferenceRPUConfig()\n",
    "    rpu_config.forward.out_res = -1.  # Turn off (output) ADC discretization.\n",
    "    rpu_config.forward.w_noise_type = WeightNoiseType.ADDITIVE_CONSTANT\n",
    "    rpu_config.forward.w_noise = 0.02  # Short-term w-noise.\n",
    "\n",
    "    rpu_config.clip.type = WeightClipType.FIXED_VALUE\n",
    "    rpu_config.clip.fixed_value = 1.0\n",
    "    rpu_config.modifier.pdrop = 0.03  # Drop connect.\n",
    "    rpu_config.modifier.type = WeightModifierType.ADD_NORMAL  # Fwd/bwd weight noise.\n",
    "    rpu_config.modifier.std_dev = 0.1\n",
    "    rpu_config.modifier.rel_to_actual_wmax = True\n",
    "\n",
    "    # Inference noise model.\n",
    "    rpu_config.noise_model = PCMLikeNoiseModel(g_max=25.0)\n",
    "\n",
    "    # drift compensation\n",
    "    rpu_config.drift_compensation = GlobalDriftCompensation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljVHauVQ9Aul"
   },
   "source": [
    "Now we'll talk about data processing. The dataset we'll be using is a copy of 'The Adventures of Sherlock Holmes' by Sir Arthur Connan Doyle. The goal of this network will be to predict the next word given a sequence of multiple words that came before it.\n",
    "\n",
    "First, we'll perform some basic preprocessing of the dataset. We'll add a space in between all punctuation characters and also convert each character to lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYI1PG0C2FEZ"
   },
   "outputs": [],
   "source": [
    "def process_data(filename):\n",
    "    data = open(filename, 'r').read()\n",
    "    \n",
    "    #first remove all extra newline characters\n",
    "    data = data.replace(\"\\n\", \"\")\n",
    "\n",
    "    #next we add a space before and after each punctuation character\n",
    "    data = re.sub('([.,!?\"\\'()])', r' \\1 ', data)\n",
    "    data = re.sub(r'\\s+', ' ', data) #remove extra spaces between words/punctuation\n",
    "\n",
    "    #for ease of processing, we'll convert all characters to lower case\n",
    "    data = data.lower()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UKMwwc79IEV"
   },
   "source": [
    "Next, we need to find a way to vectorize the inputs before feeding them into the neural network. We'll use a technique called one-hot encoding, and the steps are as follows:\n",
    "\n",
    "1. Split the dataset on empty spaces and find the most common tokens (words or puntuation). The number of tokens we want to include will determine our vocab size.\n",
    "2. Create a mapping between words in our vocab to a number. For words that aren't in our vocab (words with a low occurence), we'll map them with an 'UNK', or unknown, token instead. \n",
    "3. Iterate through each sentence in our dataset and replace the word with its corresponding number in the mapping we created previously. \n",
    "\n",
    "Converting a number to it's one hot embedding is quite simple. The embedding is a vector with 0s everywhere except the index represented by that number, which contains a 1. Take the following example for instance:\n",
    "\n",
    "$$\\begin{bmatrix} 2 & 0 & 1 & 3 \\end{bmatrix}$$ would be converted to: $$\\begin{bmatrix} 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "In the vectorize_data function, we'll simply convert each word into its corresponding number. We'll convert these numbers to it's one-hot embedding in our training loop to conserve memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jKszGBA9t9-"
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data, vocab_size, seq_len):\n",
    "    #get frequency of each word/punctuation\n",
    "    words = data.split()\n",
    "    freq_dict = Counter(words)\n",
    "    vocab = [\"UNK\"] + [word[0] for word in freq_dict.most_common(vocab_size-1)] #only select the most common words\n",
    "\n",
    "    #create one-hot encoding dictionaries\n",
    "    word2idx = dict((word, idx) for idx, word in enumerate(vocab))\n",
    "    idx2word = dict((idx, word) for idx, word in enumerate(vocab))\n",
    "\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    for seq in range(0, len(words)-seq_len-1):\n",
    "        data_sample = words[seq:seq+seq_len]\n",
    "        next_word = words[seq+seq_len]\n",
    "\n",
    "        #convert each word to its integer index if it exists. otherwise, it will be replace with \"UNK\"\n",
    "        vectorized_data_sample = [word2idx[word] if word in word2idx else word2idx[\"UNK\"] for word in data_sample]\n",
    "        vectorized_next_word = [word2idx[next_word] if next_word in word2idx else word2idx[\"UNK\"]]\n",
    "\n",
    "        features.append(vectorized_data_sample)\n",
    "        labels.append(vectorized_next_word)\n",
    "\n",
    "    #shuffle the samples\n",
    "    p = np.random.permutation(len(features))\n",
    "    features = np.array(features)[p]\n",
    "    labels = np.array(labels)[p]\n",
    "\n",
    "    return features, labels, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZrd_Kx69zYV"
   },
   "source": [
    "Below, we create our GRU neural network. It consists of an embedding layer, a GRU layer, and a decoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sVUoBzV2HEc"
   },
   "outputs": [],
   "source": [
    "class AnalogRNNNetwork(AnalogSequential):\n",
    "    \"\"\"Analog Bidirectional RNN Network definition using AnalogLinear for embedding and decoder.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATIO)\n",
    "        self.embedding = AnalogLinear(VOCAB_SIZE, EMBED_SIZE, rpu_config=rpu_config)\n",
    "        self.rnn = AnalogRNN(RNN_CELL, EMBED_SIZE, HIDDEN_SIZE, bidir=WITH_BIDIR, num_layers=NUM_LAYERS,\n",
    "                               dropout=DROPOUT_RATIO, bias=True,\n",
    "                               rpu_config=rpu_config)\n",
    "        if WITH_BIDIR:\n",
    "            self.decoder = AnalogLinear(2*HIDDEN_SIZE, VOCAB_SIZE, bias=True)\n",
    "        else:\n",
    "            self.decoder = AnalogLinear(HIDDEN_SIZE, VOCAB_SIZE, bias=True)\n",
    "\n",
    "    def forward(self, x_in, in_states=None):  # pylint: disable=arguments-differ\n",
    "        embed = self.dropout(self.embedding(x_in))\n",
    "        out, out_states = self.rnn(embed, in_states)\n",
    "\n",
    "        #to predict the output, we'll use the final hidden state of the final layer\n",
    "\n",
    "        final_layer_state = out_states[-1]\n",
    "\n",
    "        if WITH_BIDIR: #concat the forward and backward states\n",
    "            states = torch.cat((final_layer_state[0], final_layer_state[1]), dim=-1)\n",
    "        else: #only use the forward state\n",
    "            states = final_layer_state[0]\n",
    "\n",
    "        out = self.dropout(self.decoder(states))\n",
    "\n",
    "        return out, out_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFoI81nE3DNr",
    "outputId": "991ee1c2-6390-434e-f664-71a4fe379078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 27 00:36:05 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqwbLNH0-KAM"
   },
   "source": [
    "Here, we'll load in our data and instantiate our AnalogGRU model. By using the AnalogSGD optimizer, we can achieve either Analog training or Hardware Aware training, which is defined by our RPU configuration. For this example, we've configured it to use HW Aware training. \n",
    "\n",
    "The training loop grabs a batch and converts the features and labels to their appropriate one-hot encoding. Then, we perform the forward and backward passes and update our analog weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Vin6ZhG2I-h",
    "outputId": "dcb64636-f3c2-4f6a-ed3e-0f65be5b853a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1580/1580 [09:20<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0: Train Perplexity = 3.477093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1580/1580 [09:16<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1: Train Perplexity = 3.008308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1580/1580 [09:15<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2: Train Perplexity = 2.740196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1580/1580 [09:13<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3: Train Perplexity = 2.570108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1580/1580 [09:13<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4: Train Perplexity = 2.454168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1580/1580 [09:13<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5: Train Perplexity = 2.370631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1580/1580 [09:13<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6: Train Perplexity = 2.308246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1580/1580 [09:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7: Train Perplexity = 2.260464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1580/1580 [09:13<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8: Train Perplexity = 2.222378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1580/1580 [09:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9: Train Perplexity = 2.192434\n",
      "Saving Trained Model\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = process_data('/content/gdrive/MyDrive/sherlock_holmes.txt') #change this to the correct filepath!\n",
    "features, labels, word2idx, idx2word = vectorize_data(cleaned_data, VOCAB_SIZE, SEQ_LEN)\n",
    "num_train, num_test = int(0.8 * len(features)), int(0.2 * len(features))\n",
    "\n",
    "#create our training and testing sets\n",
    "train_features, test_features = features[:num_train], features[-num_test:]\n",
    "train_labels, test_labels = labels[:num_train], labels[-num_test:]\n",
    "\n",
    "model = AnalogRNNNetwork().cuda()\n",
    "optimizer = AnalogSGD(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer.regroup_param_groups(model)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# train\n",
    "losses = []\n",
    "for i in range(EPOCHS):\n",
    "    loss = 0\n",
    "    for batch in tqdm(range(0, len(train_features), BATCH_SIZE)):\n",
    "        batch_features = torch.Tensor(train_features[batch:batch+BATCH_SIZE]).long()\n",
    "        batch_labels = torch.Tensor(train_labels[batch:batch+BATCH_SIZE]).long()\n",
    "\n",
    "        #we want the input to be of shape (SEQ_LEN, BATCH_SIZE, VOCAB_SIZE)\n",
    "        batch_features = torch.transpose(batch_features, 0, 1) \n",
    "\n",
    "        #one hot encode the inputs\n",
    "        batch_one_hot_features = F.one_hot(batch_features, num_classes=VOCAB_SIZE).float().cuda()\n",
    "        batch_one_hot_labels = F.one_hot(batch_labels, num_classes=VOCAB_SIZE).squeeze().float().cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred, states = model(batch_one_hot_features)\n",
    "\n",
    "        step_loss = criterion(pred, batch_one_hot_labels)\n",
    "\n",
    "        loss += step_loss\n",
    "\n",
    "        step_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch = %d: Train Perplexity = %f' % (i, np.exp(loss.detach().cpu().numpy())))\n",
    "\n",
    "print(\"Saving Trained Model\")\n",
    "torch.save(model, '/content/gdrive/MyDrive/saved_analog_lstm.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN0n8s4U-Wx8"
   },
   "source": [
    "We finally get to the exciting part: writing some sentences! To start, we take a random snippet from our test_features. This will be our starting sentence and our model will build off of this by predicting the next word. Since this model will be autoregressive, it will use it's previous prediction to predict the words after it.\n",
    "\n",
    "To find our prediction, we will find the index of the largest probability in the output vector and use our reverse lookup table to find what word that index corresponds to. We'll append this prediction to our sentence and repeat the steps until we have predicted 30 words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VIsMls462MQy",
    "outputId": "9cb6491b-e628-424a-db2d-223b11a845e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find her eyes fixed upon me with a most searching gaze . she said nothing , but i am convinced that she had UNK that i had a UNK in my hand . remarks . wished i cold ezekiah forming parents desperate \" . energy winchester peering i defined supper arranged absurdly . stark individuality obvious patted whimsical ship ball barred wilson\n",
      "\n",
      "\n",
      "\n",
      ". \" \" alas ! \" replied our visitor , \" the very horror of my situation lies in the fact that my fears are so vague , and my suspicions depend entering minute him; glitter trace vanished thoughtfully discovering jewels friday see certainty the wet the . i command encyclopaedia the year extending lay . . waste . contains shelf .\n",
      "\n",
      "\n",
      "\n",
      ", that is it . \" it was a widespread , comfortable-looking building , two-storied , UNK , with great yellow blotches of UNK upon the grey walls . the drawn blinds rush barque vacancies the broken died painful armchair shattered the on \" impunity constable spotted shop appearance repeated heavily stoner complete aloud purpose flowers . . happily extending the weighed\n",
      "\n",
      "\n",
      "\n",
      "the cab , and i shall be with you presently . \" it was about ten minutes before we regained our cab and drove back into ross , holmes still carrying with pluck acted i manager . wilson plays i pull . isn dusk encyclopaedia . solemn read assistance the indoors hangs link the to epistle hook wired maybe two-storied lining blanche\n",
      "\n",
      "\n",
      "\n",
      "companion shook his head and shrugged his shoulders . \" no good news ? \" \" none . \" \" no bad ? \" \" no . \" \" thank god for . \" . servants . shaken ruin , . , . possessed further coroner: few really flushing sequence hook greeting freely written instincts key finally compunction sleeves frisco inquiries through\n",
      "\n",
      "\n",
      "\n",
      "the UNK are usually the more interesting . this looks like one of those UNK social UNK which call upon a man either to be bored or to lie . \" he . palm kind . hudson exaggerated someone . downward nearly wound suite summer . . . places the curt father jones lamp happened stare morcar fingertips jumped , exactly left\n",
      "\n",
      "\n",
      "\n",
      ". \" \" but to whom ? \" \" to an english lawyer named norton . \" \" but she could not love him . \" \" i am in hopes that retire tinted 000 killed undoubtedly talking merely thick ghastly , . hereford merit the . fuller weather kind . , winchester uncle . . , flushing roar warnings son; .\n",
      "\n",
      "\n",
      "\n",
      "i fear that you have felt the draught . ' \" ' on the contrary , ' said i , ' i opened the door myself because i felt the room to mark themselves servants perhaps adviser accused wink . brains . jones UNK delighted . smallest , plays the scared mingled . shaken traveller . den holborn smiled suite agency highroad\n",
      "\n",
      "\n",
      "\n",
      "called your attention to the UNK long had he been with you ? \" \" about a month then . \" \" how did he come ? \" \" in answer to encyclopaedia . civil . failing encyclopaedia watching knowing freedom recalled sharply carefully . the awful disappointed sequence the cards seems ceiling employer specimen fingertips strongest hereford parents the flowers attempt\n",
      "\n",
      "\n",
      "\n",
      "a long UNK of imprisonment and afterwards returned to england a morose and disappointed man . \" when dr . roylott was in india he married my mother , mrs . stoner , UNK caught inexplicable charming young stepped earning adviser thoroughfare compelled fitted previous the fool encyclopaedia wants link exchange ways . beckoning flowers i pale hampshire maiden . vows sofa\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model = torch.load('/content/gdrive/MyDrive/saved_analog_lstm.pt')\n",
    "#write some sentences!\n",
    "PREDICT_LENGTH = 30\n",
    "for sentence in range(10):\n",
    "    rand_idx = np.random.randint(low=0, high=len(test_features)-1)\n",
    "    sentence = list(test_features[rand_idx]) #start with a sentence from the test set\n",
    "\n",
    "    for i in range(PREDICT_LENGTH):\n",
    "        #after one hot encoding, the input shape should be (SEQ_LEN, 1, VOCAB_SIZE)\n",
    "        batch_features = torch.Tensor(sentence[i:]).long()\n",
    "        batch_features = torch.unsqueeze(batch_features, 0)\n",
    "        batch_features = torch.transpose(batch_features, 0, 1)\n",
    "        batch_one_hot_features = F.one_hot(batch_features, num_classes=VOCAB_SIZE).float().cuda()\n",
    "\n",
    "        #predict the next word and add it to the existing sentence\n",
    "        pred, _ = model(batch_one_hot_features)\n",
    "        idx = int(torch.argmax(pred[0])) \n",
    "        sentence.append(idx)\n",
    "\n",
    "    print(\" \".join([idx2word[idx] for idx in sentence]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfG6j5aP-Zfl"
   },
   "source": [
    "# Next Steps\n",
    "From the results, we can see that the current model doesn't always produce coherent sentences. There are a few things we can try to improve performance: \n",
    "1. Increase the training data by adding another book by Sir Arthur Conan Doyle\n",
    "2. Modify the model architecture (using LSTMS, adding more layers, larger hidden size, etc.)\n",
    "3. Instead of using a bag-of-words model, which relies on an UNK token, we can use a different encoding mechanism, such as byte pair encoding (more on this [here](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10))\n",
    "4. Another exciting architecture for NLP is the [Transformer](https://arxiv.org/abs/1706.03762)! And what's even better is that it can also be greatly accelerated using Analog AI. \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
