{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3S0YWetZ-a2m"
   },
   "source": [
    "# IBM Analog Hardware Acceleration Kit (AIHWKIT): Inference using noise models characterized on the IBM HERMES Project Chip\n",
    "\n",
    "Le Gallo, M., Khaddam-Aljameh, R., Stanisavljevic, M. et al. A 64-core mixed-signal in-memory compute chip based on phase-change memory for deep neural network inference. Nat Electron 6, 680â€“693 (2023). https://doi.org/10.1038/s41928-023-01010-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "8dRBAFI2xcEK",
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# various utility functions\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class LambdaLayer(torch.nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option=\"A\"):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = torch.nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == \"A\":\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(\n",
    "                        x[:, :, ::2, ::2],\n",
    "                        (0, 0, 0, 0, planes // 4, planes // 4),\n",
    "                        \"constant\",\n",
    "                        0,\n",
    "                    )\n",
    "                )\n",
    "            elif option == \"B\":\n",
    "                self.shortcut = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_planes,\n",
    "                        self.expansion * planes,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                    torch.nn.BatchNorm2d(self.expansion * planes),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self, block, num_blocks, n_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = torch.nn.Linear(64, n_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet32(n_classes=10):\n",
    "    return ResNet(BasicBlock, [5, 5, 5], n_classes=n_classes)\n",
    "\n",
    "\n",
    "class TorchCutout(object):\n",
    "    def __init__(self, length, fill=(0.0, 0.0, 0.0)):\n",
    "        self.length = length\n",
    "        self.fill = torch.tensor(fill).reshape(shape=(3, 1, 1))\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "        y1 = np.clip(y - self.length // 2, 0, h)\n",
    "        y2 = np.clip(y + self.length // 2, 0, h)\n",
    "        x1 = np.clip(x - self.length // 2, 0, w)\n",
    "        x2 = np.clip(x + self.length // 2, 0, w)\n",
    "        img[:, y1:y2, x1:x2] = self.fill\n",
    "        return img\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "def load_cifar10(batch_size, path):\n",
    "    transform_train = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.RandomCrop(32, padding=4),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            TorchCutout(length=8),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    transform_test = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=path, train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=path, train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=1\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=1\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9m1qDEsd-C4H"
   },
   "outputs": [],
   "source": [
    "# - Generic imports\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from aihwkit.inference.compensation.drift import GlobalDriftCompensation\n",
    "from aihwkit.inference.noise.hermes import HermesNoiseModel\n",
    "from aihwkit.inference.noise.pcm import PCMLikeNoiseModel\n",
    "\n",
    "# - AIHWKIT related imports\n",
    "from aihwkit.nn.conversion import convert_to_analog\n",
    "from aihwkit.simulator.configs import InferenceRPUConfig\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    BoundManagementType,\n",
    "    NoiseManagementType,\n",
    "    WeightClipType,\n",
    "    WeightModifierType,\n",
    "    WeightRemapType,\n",
    ")\n",
    "from aihwkit.simulator.presets import StandardHWATrainingPreset\n",
    "from aihwkit.simulator.presets.utils import IOParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPUConfig\n",
    "To use the Hermes noise model, adjust the `rpu_config.noise_model` field of the `RPUConfig`. The noise model can be instatiated by the class `HermesNoiseModel`. See the following cells for available options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHmjiuKn-C4O"
   },
   "outputs": [],
   "source": [
    "def gen_rpu_config(noise_model):\n",
    "    rpu_config = InferenceRPUConfig()\n",
    "\n",
    "    # To select the Hermes noise model, change the `rpu_config.noise_model` field\n",
    "    # with an instance of the noise class (see next cells for details)\n",
    "    rpu_config.noise_model = noise_model\n",
    "\n",
    "    # RPU config options to match the training config\n",
    "    rpu_config.modifier.std_dev = 0.06\n",
    "    rpu_config.modifier.type = WeightModifierType.ADD_NORMAL\n",
    "\n",
    "    rpu_config.mapping.digital_bias = True\n",
    "    rpu_config.mapping.weight_scaling_omega = 1.0\n",
    "    rpu_config.mapping.weight_scaling_columnwise = False\n",
    "    rpu_config.mapping.out_scaling_columnwise = False\n",
    "    rpu_config.remap.type = WeightRemapType.LAYERWISE_SYMMETRIC\n",
    "\n",
    "    rpu_config.clip.type = WeightClipType.LAYER_GAUSSIAN\n",
    "    rpu_config.clip.sigma = 2.0\n",
    "\n",
    "    rpu_config.forward = IOParameters()\n",
    "    rpu_config.forward.is_perfect = False\n",
    "    rpu_config.forward.out_noise = 0.0\n",
    "    rpu_config.forward.inp_bound = 1.0\n",
    "    rpu_config.forward.inp_res = 1 / (2**8 - 2)\n",
    "    rpu_config.forward.out_bound = 12\n",
    "    rpu_config.forward.out_res = 1 / (2**8 - 2)\n",
    "    rpu_config.forward.bound_management = BoundManagementType.NONE\n",
    "    rpu_config.forward.noise_management = NoiseManagementType.NONE\n",
    "\n",
    "    rpu_config.pre_post.input_range.enable = True\n",
    "    rpu_config.pre_post.input_range.decay = 0.01\n",
    "    rpu_config.pre_post.input_range.init_from_data = 50\n",
    "    rpu_config.pre_post.input_range.init_std_alpha = 3.0\n",
    "    rpu_config.pre_post.input_range.input_min_percentage = 0.995\n",
    "    rpu_config.pre_post.input_range.manage_output_clipping = False\n",
    "    \n",
    "    rpu_config.drift_compensation = GlobalDriftCompensation()\n",
    "    return rpu_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_She6Vv-C4P"
   },
   "outputs": [],
   "source": [
    "# Function to perform inference on the test set and calculate the test accuracy\n",
    "def test_step(model, criterion, testloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0-82vyr-C4Q",
    "outputId": "05431c47-efa4-4ea6-e759-5e3b5b93f66b"
   },
   "outputs": [],
   "source": [
    "# - Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# - Get the dataloader\n",
    "batch_size = 128\n",
    "_, testloader = load_cifar10(\n",
    "    batch_size=batch_size, path=os.path.expanduser(\"~/Data/\")\n",
    ")\n",
    "\n",
    "# - Define model and the criterion\n",
    "model = resnet32()\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hermes Noise Model\n",
    "Hermes' unit cell offers the capability to map a weight on either 1 device or 2 devices per polarity. Both modes have been characterized and can be called using the `num_devices` parameter during instatiation. The user has the capability to further tweek the model by changing other parameters, see the class prototype for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CF6ddwgw-C4S",
    "outputId": "17dbd555-d243-4cac-eed6-c8652d8e79bb"
   },
   "outputs": [],
   "source": [
    "# - Noise model instatiation for comparison (Previous PCMLikeNoiseModel with the new HermesNoiseModel for 1 and 2 num_devices)\n",
    "noise_models_to_compare = {\n",
    "    \"Standard\": PCMLikeNoiseModel(g_max=25.0),\n",
    "    \"Hermes 1D\": HermesNoiseModel(num_devices=1),\n",
    "    \"Hermes 2D\": HermesNoiseModel(num_devices=2),\n",
    "}\n",
    "rpu_configs = {\n",
    "    model_name: gen_rpu_config(noise_model)\n",
    "    for model_name, noise_model in noise_models_to_compare.items()\n",
    "}\n",
    "# - Instatiate models, each with an RPU config with the corresponding noise model\n",
    "analog_models = {\n",
    "    model_name: convert_to_analog(model, config) for model_name, config in rpu_configs.items()\n",
    "}\n",
    "\n",
    "# Download the HW-Aware trained checkpoint and load it in the models\n",
    "!wget -P Models/ https://aihwkit-tutorial.s3.us-east.cloud-object-storage.appdomain.cloud/finetuned_model_0.9.1.th\n",
    "for model in analog_models.values():\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"Models/finetuned_model_0.9.1.th\", map_location=device), load_rpu_config=False,\n",
    "    )\n",
    "print(f\"Finetuned test acc. w/o noise: {test_step(analog_models['Standard'], criterion, testloader):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uAF1fy68-C4V",
    "outputId": "c5e5762b-cb7b-4dba-8ff9-a33bdd885468"
   },
   "outputs": [],
   "source": [
    "# - For programming the model, we need to put it into eval() mode\n",
    "for model in analog_models.values():\n",
    "    model.eval()\n",
    "# - We repeat each measurement 5 times\n",
    "n_rep = 5\n",
    "t_inferences = [0.0, 60.0, 3600.0, 86400.0, 2592000.0, 31104000.0]\n",
    "_, ax = plt.subplots()\n",
    "ax: plt.Axes\n",
    "for noise_name, model in analog_models.items():\n",
    "    drifted_test_accs = torch.zeros(size=(len(t_inferences), n_rep))\n",
    "    for i, t in enumerate(t_inferences):\n",
    "        for j in range(n_rep):\n",
    "            model.drift_analog_weights(t)\n",
    "            drifted_test_accs[i, j] = test_step(model, criterion, testloader)\n",
    "\n",
    "    ax.errorbar(\n",
    "        t_inferences,\n",
    "        drifted_test_accs.mean(1),\n",
    "        drifted_test_accs.std(1),\n",
    "        capsize=3,\n",
    "        label=noise_name,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_ylabel(\"Test acc. (%)\")\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "8552d10987f98ef8320154686d1598142476aa8dfb1019e7b5164f51d5b1d29f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
