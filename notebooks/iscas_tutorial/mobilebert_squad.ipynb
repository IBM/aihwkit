{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S0YWetZ-a2m"
      },
      "source": [
        "# Tutorial: Hardware-aware Training and Hyper-parameter Optimiation for MobileBERT / SQuADv1.1\n",
        "\n",
        "### Authors: [Corey Lammie](https://www.linkedin.com/in/coreylammie/)\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/IBM/aihwkit/blob/master/notebooks/iscas-tutorial/mobilebert_squad.ipynb\" target=\"_parent\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\"/>\n",
        "</a>\n",
        "\n",
        "The IBM Analog Hardware Acceleration Kit (AIHWKIT) is an open-source Python toolkit for exploring and using the capabilities of in-memory computing devices (PCM, RRAM and others) in the context of artificial intelligence. The PyTorch integration consists of a series of primitives and features that allow using the toolkit within PyTorch.\n",
        "The GitHub repository can be found at: https://github.com/IBM/aihwkit\n",
        "To learn more about Analog AI and the harware befind it, refer to this webpage: https://aihw-composer.draco.res.ibm.com/about\n",
        "\n",
        "This notebook demonstrates how AIHWKIT can be used in conjunction with [W&B](https://wandb.ai/site) to perform hyper-parameter optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cEEFk_IxSiM"
      },
      "source": [
        "## Install the AIHWKIT\n",
        "This tutorial assumes that you have installed the AIHWKIT. If you have not, it can be installed by commenting out lines in the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEfolvgr-HSO"
      },
      "outputs": [],
      "source": [
        "# To install the cpu-only enabled kit, un-comment the line below\n",
        "#!pip install aihwkit\n",
        "\n",
        "# To install the GPU-enabled wheel, un-comment the lines below\n",
        "# !wget https://aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud/aihwkit-0.9.0+cuda117-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "# !pip install aihwkit-0.9.0+cuda117-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05XKOcQdoWis"
      },
      "source": [
        "## Install other Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8dRBAFI2xcEK",
        "jupyter": {
          "source_hidden": true
        },
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "!pip install wandb accelerate transformers tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHHRl-duxSiO"
      },
      "source": [
        "## Authenticate W&B\n",
        "If you do not already have a W&B account, please create one [here](https://wandb.ai/site)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m1qDEsd-C4H"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJpJeStD-C4M"
      },
      "source": [
        "## Define a RPU Config\n",
        "The RPU configuration specifies the parameters necessary for hardware-aware training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DHmjiuKn-C4O"
      },
      "outputs": [],
      "source": [
        "import aihwkit\n",
        "from aihwkit.simulator.presets.inference import StandardHWATrainingPreset\n",
        "\n",
        "\n",
        "rpu_config = StandardHWATrainingPreset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X43dbg-9xSiP"
      },
      "source": [
        "## Define a Configururation File Describing the Training Configurations and Parameters to Optimize\n",
        "The following attributes decribe how the optimization is performed:\n",
        "* **method**: The optimization method to use (Bayesian Optimization/Grid search/etc.).\n",
        "* **metirc**: The optimization goal and item.\n",
        "\n",
        "A full decription of each parameter is as follows:\n",
        "* **logging_step_frequency**: The interval (number of steps) to perform logging for.\n",
        "* **max_seq_len**: The maximum sequence length of the transformer model (MobileBERT).\n",
        "* **batch_size_train**: The batch size used during training.\n",
        "* **batch_size_eval**: The batch size used during evaluation.\n",
        "* **weight_decay**: The L2 weight decay parameter (used during training).\n",
        "* **num_training_epochs**: The number of fine-tuning/training epochs.\n",
        "* **learning_rate**: The learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6_She6Vv-C4P"
      },
      "outputs": [],
      "source": [
        "configuration = \"\"\"\n",
        "method: bayes\n",
        "early_terminate:\n",
        "  min_iter: 100\n",
        "  type: hyperband\n",
        "metric:\n",
        "  goal: minimize\n",
        "  name: training_loss\n",
        "parameters:\n",
        "  logging_step_frequency:\n",
        "    value: 5\n",
        "  max_seq_length:\n",
        "    value: 320\n",
        "  batch_size_train:\n",
        "    value: 16\n",
        "  batch_size_eval:\n",
        "    value: 32\n",
        "  weight_decay:\n",
        "    value: 0.0005\n",
        "  num_training_epochs:\n",
        "    value: 1\n",
        "  learning_rate:\n",
        "    distribution: uniform\n",
        "    max: 6.0 # 10 ** -max\n",
        "    min: 1.0 # 10 ** -min\n",
        "\"\"\"\n",
        "with open(\"configuration.yaml\", \"w\") as file:\n",
        "    file.write(configuration[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_dVDbztxSiP"
      },
      "source": [
        "## Include Utility Functions to Load the Model and Dataset/Perform Training/Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P0-82vyr-C4Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    squad_convert_examples_to_features,\n",
        ")\n",
        "from transformers.data.metrics.squad_metrics import (\n",
        "    compute_predictions_logits,\n",
        "    squad_evaluate,\n",
        ")\n",
        "from transformers.data.processors.squad import SquadResult, SquadV1Processor\n",
        "\n",
        "\n",
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n",
        "\n",
        "def load_and_cache_examples(\n",
        "    model_name_or_path,\n",
        "    tokenizer,\n",
        "    max_seq_length,\n",
        "    evaluate=False,\n",
        "    output_examples=False,\n",
        "    overwrite_cache=False,\n",
        "    cache_dir=\"data\",\n",
        "):\n",
        "    cached_features_file = os.path.join(\n",
        "        \"cached_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(max_seq_length),\n",
        "        ),\n",
        "    )\n",
        "    if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "        import tensorflow_datasets as tfds\n",
        "\n",
        "        tfds_examples = tfds.load(\"squad\", data_dir=cache_dir)\n",
        "        examples = SquadV1Processor().get_examples_from_dataset(\n",
        "            tfds_examples, evaluate=evaluate\n",
        "        )\n",
        "        features, dataset = squad_convert_examples_to_features(\n",
        "            examples=examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=max_seq_length,\n",
        "            doc_stride=128,\n",
        "            max_query_length=64,\n",
        "            is_training=not evaluate,\n",
        "            return_dataset=\"pt\",\n",
        "            threads=8,\n",
        "        )\n",
        "        torch.save(\n",
        "            {\"features\": features, \"dataset\": dataset, \"examples\": examples},\n",
        "            cached_features_file,\n",
        "        )\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    examples,\n",
        "    features,\n",
        "    eval_dataloader,\n",
        "    cache_dir,\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    early_exit_n_iters=-1,\n",
        "):\n",
        "    all_results = []\n",
        "    batch_idx = 0\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "            feature_indices = batch[3]\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        for i, feature_index in enumerate(feature_indices):\n",
        "            eval_feature = features[feature_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "            output = [to_list(output[i]) for output in outputs.to_tuple()]\n",
        "            if len(output) >= 5:\n",
        "                start_logits = output[0]\n",
        "                start_top_index = output[1]\n",
        "                end_logits = output[2]\n",
        "                end_top_index = output[3]\n",
        "                cls_logits = output[4]\n",
        "                result = SquadResult(\n",
        "                    unique_id,\n",
        "                    start_logits,\n",
        "                    end_logits,\n",
        "                    start_top_index=start_top_index,\n",
        "                    end_top_index=end_top_index,\n",
        "                    cls_logits=cls_logits,\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                start_logits, end_logits = output\n",
        "                result = SquadResult(unique_id, start_logits, end_logits)\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "        if batch_idx == early_exit_n_iters:\n",
        "          break\n",
        "\n",
        "        batch_idx += 1\n",
        "\n",
        "    output_prediction_file = os.path.join(cache_dir, \"predictions.json\")\n",
        "    output_nbest_file = os.path.join(cache_dir, \"nbest_predictions.json\")\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples[0:early_exit_n_iters],\n",
        "        features[0:early_exit_n_iters],\n",
        "        all_results[0:early_exit_n_iters],\n",
        "        20,\n",
        "        30,\n",
        "        True,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        None,\n",
        "        False,\n",
        "        False,\n",
        "        0.0,\n",
        "        tokenizer,\n",
        "    )\n",
        "    results = squad_evaluate(examples[0:early_exit_n_iters], predictions)\n",
        "    return results\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    train_dataloader,\n",
        "    model,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    current_epoch,\n",
        "    logging_step_frequency,\n",
        "    wandb_logging=False,\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    early_exit_n_iters=-1,\n",
        "):\n",
        "    model.train()\n",
        "    step = 0\n",
        "    n_steps = len(train_dataloader)\n",
        "    with tqdm(train_dataloader, desc=\"Iteration\") as tepoch:\n",
        "        for batch in tepoch:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "                \"start_positions\": batch[3],\n",
        "                \"end_positions\": batch[4],\n",
        "            }\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            if step % logging_step_frequency == 0:\n",
        "                tepoch.set_postfix(loss=loss.item(), lr=scheduler.get_lr()[0])\n",
        "                if wandb_logging:\n",
        "                    wandb.log(\n",
        "                        {\n",
        "                            \"step\": n_steps * current_epoch + step,\n",
        "                            \"training_loss\": loss.item(),\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "            if step == early_exit_n_iters:\n",
        "              break\n",
        "\n",
        "            step += 1\n",
        "\n",
        "\n",
        "\n",
        "def load_model_tokenizer(\n",
        "    model_id,\n",
        "    cache_dir=\"cache\",\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "):\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_id,\n",
        "        cache_dir=cache_dir,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_id,\n",
        "        do_lower_case=True,\n",
        "        cache_dir=cache_dir,\n",
        "        use_fast=False,\n",
        "    )\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "        model_id,\n",
        "        from_tf=False,\n",
        "        config=config,\n",
        "        cache_dir=cache_dir,\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def load_dataloader_examples_features(\n",
        "    model_id, tokenizer, evaluate, batch_size=16, max_seq_length=320,\n",
        "):\n",
        "    dataset, examples, features = load_and_cache_examples(\n",
        "        model_id, tokenizer, max_seq_length, evaluate=evaluate, output_examples=True\n",
        "    )\n",
        "    if evaluate:\n",
        "        sampler = SequentialSampler(dataset)\n",
        "    else:\n",
        "        sampler = RandomSampler(dataset)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        sampler=sampler,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    return dataloader, examples, features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7XeiSmFxSiQ"
      },
      "source": [
        "## Define a Function to Load the Model (MobileBERT) and Dataset (SQuADv1.1), and to Execute the Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aG-rrTt5-C4R"
      },
      "outputs": [],
      "source": [
        "from aihwkit.optim import AnalogAdam\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from aihwkit.nn.conversion import convert_to_analog\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def main(t_inferences=[0., 3600., 86400.], n_reps=5, early_exit_n_iters=-1):\n",
        "    wandb.init()\n",
        "    max_seq_length = wandb.config.max_seq_length\n",
        "    logging_step_frequency = wandb.config.logging_step_frequency\n",
        "    batch_size_train = wandb.config.batch_size_train\n",
        "    batch_size_eval = wandb.config.batch_size_eval\n",
        "    weight_decay = wandb.config.weight_decay\n",
        "    num_training_epochs = wandb.config.num_training_epochs\n",
        "    learning_rate = 10 ** -wandb.config.learning_rate\n",
        "    model_id = \"csarron/mobilebert-uncased-squad-v1\"\n",
        "    model, tokenizer = load_model_tokenizer(model_id, \"data\")\n",
        "    print(\"Loading and parsing training features.\")\n",
        "    train_dataloader, train_examples, train_features = load_dataloader_examples_features(model_id, tokenizer, evaluate=False, batch_size=batch_size_train, max_seq_length=max_seq_length)\n",
        "    print(\"Loading and parsing evaluation features.\")\n",
        "    test_dataloader, test_examples, test_features = load_dataloader_examples_features(model_id, tokenizer, evaluate=True, batch_size=batch_size_eval, max_seq_length=max_seq_length)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    model = convert_to_analog(model, rpu_config, verbose=False)\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [\n",
        "                p\n",
        "                for n, p in model.named_parameters()\n",
        "                if not any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [\n",
        "                p\n",
        "                for n, p in model.named_parameters()\n",
        "                if any(nd in n for nd in no_decay)\n",
        "            ],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = AnalogAdam(\n",
        "        optimizer_grouped_parameters, lr=learning_rate,\n",
        "    )\n",
        "    t_total = len(train_dataloader) // num_training_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=0, num_training_steps=t_total\n",
        "    )\n",
        "    model.zero_grad()\n",
        "    for current_epoch in range(0, num_training_epochs):\n",
        "        print(\"Training epoch: \", current_epoch)\n",
        "        model.train()\n",
        "        train_epoch(\n",
        "            train_dataloader,\n",
        "            model,\n",
        "            optimizer,\n",
        "            scheduler,\n",
        "            current_epoch=current_epoch,\n",
        "            logging_step_frequency=logging_step_frequency,\n",
        "            wandb_logging=True,\n",
        "            early_exit_n_iters=early_exit_n_iters,\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for t in t_inferences:\n",
        "              print('t_inference:', t)\n",
        "              f1_scores = []\n",
        "              for rep in range(n_reps):\n",
        "                  model.drift_analog_weights(t)\n",
        "                  result = evaluate(model, tokenizer, test_examples, test_features, test_dataloader, cache_dir='data', early_exit_n_iters=early_exit_n_iters)\n",
        "                  f1_scores.append(result['f1'])\n",
        "\n",
        "              print(\"=====\", t, np.mean(f1_scores), np.std(f1_scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD4Y2wot-C4R"
      },
      "source": [
        "## Load the Configuration File and Execute the Optimization Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_Cvn56G-C4R"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "\n",
        "with open('configuration.yaml') as f:\n",
        "    sweep_configuration = yaml.load(f, Loader=yaml.FullLoader)\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"mobilebert_squadv1\")\n",
        "wandb.agent(sweep_id, function=main, count=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "8552d10987f98ef8320154686d1598142476aa8dfb1019e7b5164f51d5b1d29f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
