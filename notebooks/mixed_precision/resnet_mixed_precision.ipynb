{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analog-Digital Mixed-Precision Training and Inference\n",
    "\n",
    "### Authors: [Athanasios Vasilopoulos](https://www.linkedin.com/in/thanos-vasilopoulos/)\n",
    "\n",
    "In this notebook, we demonstrate the capability of performing mixed-precision training and inference in AIHWKit. The notebook uses a ResNet32 as a use case and demonstrates:\n",
    "- The conversion of an FP model to a mixed-precision analog-digital model, with per-torch-module fidelity. All activations (including outputs and affine parameters of tiles) and all digital modules have configurable precision. The quantization parameters are learnable during training or estimated post-training with a variety of range-estimators.\n",
    "- We demonstrate how to perform simultaneous training of a model with analog modules and low-precision digital modules and activations.\n",
    "- We demonstrate how to perform post-training quantization on the activations and modules of a network that was trained only in an analog-aware manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from copy import deepcopy\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from aihwkit.inference.calibration import calibrate_quantization_ranges\n",
    "from aihwkit.inference.compensation.drift import GlobalDriftCompensation\n",
    "from aihwkit.inference.noise.pcm import PCMLikeNoiseModel\n",
    "from aihwkit.nn.conversion import convert_to_analog\n",
    "from aihwkit.nn.low_precision_conversion import convert_to_quantized\n",
    "from aihwkit.nn.low_precision_modules.conversion_utils import append_default_conversions\n",
    "from aihwkit.nn.low_precision_modules.quantized_base_modules import (\n",
    "    QuantBatchNorm2d,\n",
    "    QuantConv2d,\n",
    "    QuantLinear,\n",
    ")\n",
    "from aihwkit.optim import AnalogSGD\n",
    "from aihwkit.simulator.configs import QuantizedTorchInferenceRPUConfig, TorchInferenceRPUConfig\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    BoundManagementType,\n",
    "    NoiseManagementType,\n",
    "    WeightClipType,\n",
    "    WeightModifierType,\n",
    "    WeightRemapType,\n",
    ")\n",
    "from aihwkit.simulator.parameters.quantization import (\n",
    "    ActivationQuantConfig,\n",
    "    QuantizationMap,\n",
    "    QuantizedModuleConfig,\n",
    ")\n",
    "from aihwkit.simulator.presets.utils import IOParameters\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility imports - model definition, training/test steps, dataset preparation\n",
    "from model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPU config generation for either the quantized or traditional TorchTile.\n",
    "# The differences involve the handling of DACs, as well as the definition of\n",
    "# parameters for the configuration of the new quantized periphery.\n",
    "def gen_rpu_config(quant_tile, n_act_bits, asym_act):\n",
    "    # If quant_tile is True, the QuantizedTorchInferenceRPUConfig is initialized which means\n",
    "    # that the QuantizedTorchInferenceTile is used for inference. The QuantizedTorchInferenceTile\n",
    "    # is a wrapper of TorchInferenceTile which extends its functionality with quantized outputs \n",
    "    # (post-ADC) and quantized periphery, which applies the affine transformations and bias in \n",
    "    # reduced precision. The former is configured with the rpu_config.act_quant_config parameter \n",
    "    # and the latter with the rpu_config.pre_post.periph_quant. See below for details.\n",
    "    rpu_config = QuantizedTorchInferenceRPUConfig() if quant_tile else TorchInferenceRPUConfig()\n",
    "\n",
    "    rpu_config.modifier.std_dev = 0.06\n",
    "    rpu_config.modifier.type = WeightModifierType.ADD_NORMAL\n",
    "\n",
    "    rpu_config.mapping.digital_bias = True\n",
    "    rpu_config.mapping.weight_scaling_omega = 1.0\n",
    "    rpu_config.mapping.weight_scaling_columnwise = True\n",
    "    rpu_config.mapping.learn_out_scaling = False\n",
    "\n",
    "    rpu_config.remap.type = WeightRemapType.LAYERWISE_SYMMETRIC\n",
    "\n",
    "    rpu_config.clip.type = WeightClipType.LAYER_GAUSSIAN\n",
    "    rpu_config.clip.sigma = 2.0\n",
    "\n",
    "    rpu_config.forward = IOParameters()\n",
    "    rpu_config.forward.is_perfect = False\n",
    "    rpu_config.forward.out_noise = 0.0\n",
    "    rpu_config.forward.out_bound = 12\n",
    "    rpu_config.forward.out_res = 1 / (2**8 - 2)\n",
    "    rpu_config.forward.bound_management = BoundManagementType.NONE\n",
    "    rpu_config.forward.noise_management = NoiseManagementType.ABS_MAX\n",
    "\n",
    "    # When the quantized tile is used, the network is converted such that the \n",
    "    # input to every tile is already quantized by the layer before it. As such\n",
    "    # no DAC functionality is necessary\n",
    "    rpu_config.forward.inp_bound = 0.0 if quant_tile else 1.0\n",
    "    rpu_config.forward.inp_res = -1 if quant_tile else 1 / (2**n_act_bits - 2)\n",
    "\n",
    "    # Enable input range learning and its parameters if quant_tile is False\n",
    "    rpu_config.pre_post.input_range.enable = False if quant_tile else True\n",
    "    rpu_config.pre_post.input_range.decay = 0.01\n",
    "    rpu_config.pre_post.input_range.init_from_data = 50\n",
    "    rpu_config.pre_post.input_range.init_std_alpha = 3.0\n",
    "    rpu_config.pre_post.input_range.input_min_percentage = 0.995\n",
    "    rpu_config.pre_post.input_range.manage_output_clipping = False\n",
    "\n",
    "    rpu_config.noise_model = PCMLikeNoiseModel(g_max=25.0)\n",
    "    rpu_config.drift_compensation = GlobalDriftCompensation()\n",
    "\n",
    "    if quant_tile:\n",
    "        # Configure what the output activation precision of each tile.\n",
    "        # When a layer uses more than one tile, this option applies to\n",
    "        # the result of each individual tile AND to the final output \n",
    "        # after proper partial result accumulation and concatenation.\n",
    "        # See `QuantizedTileModuleArray` for more details.\n",
    "        rpu_config.act_quant_config = ActivationQuantConfig(\n",
    "            n_bits=n_act_bits,\n",
    "            symmetric=not asym_act,\n",
    "        )\n",
    "\n",
    "        # Configure the precision of the periphery, which applies to the\n",
    "        # affine transformations and bias additions of the tiles.\n",
    "        rpu_config.pre_post.periph_quant.n_bits = n_act_bits\n",
    "        rpu_config.pre_post.periph_quant.symmetric = True\n",
    "\n",
    "    return rpu_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to the QuantizationMap the recipe to convert any modules that are not in the\n",
    "# DEFAULT_CONVERSIONS (which includes only Linear, Conv2d, Embedding, and LayerNorm).\n",
    "# In the case of this network, the only module is BatchNorm2d. Other networks, with\n",
    "# more advanced modules, may require custom quantized layer implementations using the\n",
    "# tools we offer in this framework, like the QuantizedActivation, etc.\n",
    "def append_resnet_quantization_map(quantization_map: QuantizationMap):\n",
    "    quantization_map.module_qconfig_map[torch.nn.BatchNorm2d] = QuantizedModuleConfig(\n",
    "        quantized_module=QuantBatchNorm2d,\n",
    "        module_qconfig=deepcopy(quantization_map.default_qconfig),\n",
    "    )\n",
    "\n",
    "\n",
    "# Function to convert the pretrained model to a mixed-precision model,\n",
    "# merging the convert_to_analog and convert_to_quantized calls, with\n",
    "# the appropriate selections for the resnet network in question.\n",
    "# The tools we offer allow for the configuration of every single\n",
    "# activation in the system, enabling faithful replication of\n",
    "# the user's targetted system.\n",
    "def convert_to_mixedprecision(\n",
    "    model,  # The input model\n",
    "    mode,  # \"mixed\" will place first and last layers on digital, \"analog\" will place all weight layers in analog\n",
    "    act_quant,  # Precision of activations across the system\n",
    "    asym_act,  # Whether activations are asymmetric\n",
    "    weight_quant,  # Weight precision if \"mixed\" is selected\n",
    "    per_channel_weight=True,  # Whether weights are quantized per_channel or not\n",
    "    # Flags to perform partial convertions - Used during PTQ (see the corresponding code)\n",
    "    only_analog_conversion=False,\n",
    "    ptq=False,\n",
    "):\n",
    "    # Generate the RPU config according to the parameters. The QuantizedTorchInferenceTile\n",
    "    # is only selected if activations on the network are selected to be quantized\n",
    "    base_rpu_config = gen_rpu_config(\n",
    "        quant_tile=(act_quant > 0), n_act_bits=act_quant, asym_act=asym_act\n",
    "    )\n",
    "\n",
    "    # Convert to analog, excluding first and last layers if \"mixed\" mode is given\n",
    "    model = convert_to_analog(\n",
    "        model,\n",
    "        base_rpu_config,\n",
    "        exclude_modules=[] if mode == \"analog\" else [\"conv1\", \"linear\"],\n",
    "    )\n",
    "\n",
    "    # For PTQ we may need to stop here so that we can load an analog-only trained checkpoint\n",
    "    # before proceeding to the activation and module quantization\n",
    "    if only_analog_conversion:\n",
    "        return model\n",
    "\n",
    "    # Now we'll define the quantization configuration for all modules (excluding the already\n",
    "    # analog converted modules). We also have the capability to wrap a given module with input\n",
    "    # quantization if there's no an already quantized activation coming to it (from a functional\n",
    "    # call for example). That way we can keep a true quantized-everywhere activation flow.\n",
    "    quantization_map = QuantizationMap()\n",
    "    # Populate the default quantization config with the given parameters\n",
    "    quantization_map.default_qconfig.activation_quant.n_bits = act_quant\n",
    "    quantization_map.default_qconfig.activation_quant.symmetric = not asym_act\n",
    "    if mode == \"mixed\":\n",
    "        quantization_map.default_qconfig.weight_quant.n_bits = weight_quant\n",
    "        quantization_map.default_qconfig.weight_quant.per_channel = per_channel_weight\n",
    "\n",
    "    # As mentioned before, some layers will receive data that are not quantized from the layer\n",
    "    # before it. In this network, this happens in the first layer that receives data from the\n",
    "    # dataset directly, the last layer that receives data from a functional pooling layer and\n",
    "    # for every resnet block that receives data after a residual addition (the first resnet block\n",
    "    # receives data directly from a batchnorm so it's skipped here). With the\n",
    "    # quantization_map.input_activation_qconfig_map we can add the names of all of these layers\n",
    "    # such that we assure their input activations get quantized properly.\n",
    "    if act_quant > 0:\n",
    "        quantization_map.input_activation_qconfig_map = {\n",
    "            lname: quantization_map.default_qconfig.activation_quant\n",
    "            for lname in [\"conv1\", \"linear\"]\n",
    "            + [f\"layer{i}.{j}\" for j in range(5) for i in range(1, 4) if (i, j) != (1, 0)]\n",
    "        }\n",
    "\n",
    "    # Append the resnet and default module conversions to the quantization_map.module_qconfig_map.\n",
    "    # These conversions will use the quantization_map.default_qconfig parameters as a recipe for each\n",
    "    # of the appended conversions. If some specific layers need to use different parameters than the\n",
    "    # default_qconfig parameters, they must be defined in the quantization_map.instance_qconfig_map\n",
    "    # field by name, along with their special parameters.\n",
    "    append_resnet_quantization_map(quantization_map)\n",
    "    append_default_conversions(quantization_map)\n",
    "\n",
    "    # For the case that we convert from an analog network to a mixed network, we need to define\n",
    "    # a conversion of an Analog layer to a Quantized layer. Since we only need to do it for two\n",
    "    # layers, we define it here as instance specific conversion.\n",
    "    if mode == \"mixed\" and ptq:\n",
    "        quantization_map.instance_qconfig_map[\"conv1\"] = QuantizedModuleConfig(\n",
    "            quantized_module=QuantConv2d,\n",
    "            module_qconfig=deepcopy(quantization_map.default_qconfig),\n",
    "        )\n",
    "        quantization_map.instance_qconfig_map[\"linear\"] = QuantizedModuleConfig(\n",
    "            quantized_module=QuantLinear,\n",
    "            module_qconfig=deepcopy(quantization_map.default_qconfig),\n",
    "        )\n",
    "\n",
    "    # Convert to quantized model using all the options we configured above\n",
    "    model = convert_to_quantized(model, quantization_map)\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed-precision training and PTQ code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Pretrained test acc. 94.13%\n"
     ]
    }
   ],
   "source": [
    "# Parameters to configure how to convert the model\n",
    "# and which analog-only checkpoint to use for the PTQ demonstration\n",
    "mode = \"mixed\"  # \"analog or \"mixed\" Mixed means that first and last layers are in digital with `weight_quant` precision\n",
    "act_quant = 6   # Precision of activations in bits\n",
    "asym_act = True # Asymmetric activations or not\n",
    "weight_quant = 8 # Precision of weights (when mixed is True)\n",
    "\n",
    "\n",
    "# Training hyper params\n",
    "perform_finetuning = False  # True to perform the mixed-precision training, otherwise it will load a known-good checkpoint\n",
    "epochs_finetuning = 200 # How many epochs to finetune, if `perform_finetuning` is True\n",
    "batch_size = 128\n",
    "lr = 5e-3\n",
    "seed = 0\n",
    "\n",
    "# Evaluation repetitions and time for the analog layers\n",
    "eval_reps = 10\n",
    "eval_time = 0  # 86400\n",
    "\n",
    "# Set seeds\n",
    "set_seed(seed)\n",
    "\n",
    "# Auto-generation of a unique model name, based on the parameters\n",
    "model_name = (\n",
    "    f\"{mode}_{f'quantact{act_quant}' if act_quant>0 else 'fpact'}\"\n",
    "    + f\"{'_asym' if asym_act and act_quant>0 else ''}\"\n",
    "    + f\"{f'_quantw{weight_quant}' if weight_quant>0 and mode == 'mixed' else ''}\"\n",
    "    + f\"_lr{lr:.0e}_seed{seed}\"\n",
    ")\n",
    "model_name = f\"mixed_prec_example_{model_name}\"\n",
    "# Get the dataloader\n",
    "trainloader, testloader = load_cifar10(batch_size=batch_size, path=os.path.expanduser(\"data/\"))\n",
    "\n",
    "# Define model, criterion, optimizer and scheduler.\n",
    "petrained_model = resnet32()\n",
    "petrained_model = petrained_model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Load and evaluate the FP-only pretrained model\n",
    "if not os.path.exists(\"Models\"):\n",
    "    os.makedirs(\"Models\")\n",
    "url = 'https://aihwkit-tutorial.s3.us-east.cloud-object-storage.appdomain.cloud/mixed_prec_example_pre_trained_model.th'\n",
    "response = requests.get(url)\n",
    "with open('Models/mixed_prec_example_pre_trained_model.th', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "petrained_model.load_state_dict(\n",
    "    torch.load(\"Models/mixed_prec_example_pre_trained_model.th\", map_location=device, weights_only=True)\n",
    ")\n",
    "print(f\"Pretrained test acc. {test_step(petrained_model, criterion, testloader)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed-precision finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "# Convert the model to a mixed-precicion model for finetuning\n",
    "mixed_precision_model_fortraining = convert_to_mixedprecision(\n",
    "    petrained_model, mode, act_quant, asym_act, weight_quant\n",
    ")\n",
    "\n",
    "if perform_finetuning:\n",
    "    # Perform a normal training loop\n",
    "    optimizer_class = AnalogSGD if mode in [\"analog\", \"mixed\"] else torch.optim.SGD\n",
    "    optimizer = optimizer_class(\n",
    "        mixed_precision_model_fortraining.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs_finetuning)\n",
    "\n",
    "    best_acc = -1.0\n",
    "    pbar = tqdm(range(epochs_finetuning))\n",
    "    for epoch in pbar:\n",
    "        train_loss, train_acc = train_step(\n",
    "            mixed_precision_model_fortraining, optimizer, criterion, trainloader\n",
    "        )\n",
    "        pbar.set_description(\n",
    "            f\"Epoch {epoch} Train loss: {train_loss:.4f} train acc. {train_acc:.2f}%\"\n",
    "        )\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        test_acc = test_step(mixed_precision_model_fortraining, criterion, testloader)\n",
    "        if test_acc > best_acc and epoch > 50:\n",
    "            best_acc = test_acc\n",
    "            torch.save(\n",
    "                mixed_precision_model_fortraining.state_dict(),\n",
    "                os.path.join(f\"Models/{model_name}.th\"),\n",
    "            )\n",
    "\n",
    "        scheduler.step()\n",
    "else:\n",
    "    # Load from a known-good checkpoint for evaluation\n",
    "    assert (\n",
    "        model_name == \"mixed_prec_example_mixed_quantact6_asym_quantw8_lr5e-03_seed0\"\n",
    "    ), \"Checkpoint is only offered for one combination\"\n",
    "    url = \"https://aihwkit-tutorial.s3.us-east.cloud-object-storage.appdomain.cloud/mixed_prec_example_mixed_quantact6_asym_quantw8_lr5e-03_seed0.th\"\n",
    "    response = requests.get(url)\n",
    "    with open(f\"Models/{model_name}.th\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    mixed_precision_model_fortraining.load_state_dict(\n",
    "        torch.load(f\"Models/{model_name}.th\", weights_only=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTQ of an Analog HWA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "# First convert to just an analog model to load the state dict of the already trained analog model.\n",
    "# Doing it with strict=False and with load_rpu_config=False because we need the RPU config to be as\n",
    "# defined in the RPU config function above. The checkpoint may have trained input ranges or other\n",
    "# parameters that are going to be ignored when the rest of the network produces quantized activations.\n",
    "ana_model = convert_to_mixedprecision(\n",
    "    petrained_model, \"analog\", act_quant, asym_act, weight_quant, only_analog_conversion=True\n",
    ")\n",
    "url = \"https://aihwkit-tutorial.s3.us-east.cloud-object-storage.appdomain.cloud/mixed_prec_example_analog_fpact_lr5e-03_seed0.th\"\n",
    "response = requests.get(url)\n",
    "with open(\"Models/mixed_prec_example_analog_fpact_lr5e-03_seed0.th\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "ana_model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"Models/mixed_prec_example_analog_fpact_lr5e-03_seed0.th\",\n",
    "        weights_only=False,\n",
    "        map_location=device,\n",
    "    ),\n",
    "    strict=False,\n",
    "    load_rpu_config=False,\n",
    ")\n",
    "\n",
    "# Convert to mixed-precision but with all of the options, to convert\n",
    "# batchnorms and wrap modules with I/O quantization where applicable\n",
    "ptq_model = convert_to_mixedprecision(ana_model, mode, act_quant, asym_act, weight_quant, ptq=True)\n",
    "\n",
    "# Now that the model is loaded with the correct weights from the analog checkpoint and contains\n",
    "# all the quantizer modules, we need to calibrate them and find good quantizations scale-offsets.\n",
    "# For this reason, we offer the `calibrate_quantization_ranges` function that will use training\n",
    "# data to calibrate all the (currently uninitialized) quantizers in the ptq network.\n",
    "_ = calibrate_quantization_ranges(ptq_model, trainloader, max_num_batches=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the finetuned and PTQed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10/10 [04:42<00:00, 28.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [PTQ Model Accuracy]:\t\t92.61 +- 0.23 %\n",
      " [Trained Model Accuracy]:\t93.07 +- 0.14 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Place models in eval mode\n",
    "ptq_model.eval()\n",
    "mixed_precision_model_fortraining.eval()\n",
    "\n",
    "# Evaluate `eval_reps` times, each time resampling the analog noise\n",
    "test_accs_ptq, test_accs_training = [], []\n",
    "pbar = tqdm(range(eval_reps))\n",
    "for step in pbar:\n",
    "    # Add noise on both models with t_inference equal to `eval_time`\n",
    "    ptq_model.drift_analog_weights(t_inference=eval_time)\n",
    "    mixed_precision_model_fortraining.drift_analog_weights(t_inference=eval_time)\n",
    "    # Evaluate both models\n",
    "    test_accs_ptq.append(test_step(ptq_model, criterion, testloader))\n",
    "    test_accs_training.append(test_step(mixed_precision_model_fortraining, criterion, testloader))\n",
    "\n",
    "print(f\" [PTQ Model Accuracy]:\\t\\t{np.mean(test_accs_ptq):.2f} +- {np.std(test_accs_ptq):.2f} %\")\n",
    "print(\n",
    "    f\" [Trained Model Accuracy]:\\t{np.mean(test_accs_training):.2f} +- {np.std(test_accs_training):.2f} %\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
