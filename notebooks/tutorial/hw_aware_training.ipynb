{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3S0YWetZ-a2m"
   },
   "source": [
    "# IBM Analog Hardware Acceleration Kit (AIHWKIT): Hardware-aware Training for Accurate Inference on Analog Hardware\n",
    "\n",
    "### Authors: [Julian Büchel](https://www.linkedin.com/in/julian-büchel-0673991a3/), [Manuel Le Gallo-Bourdeau](https://research.ibm.com/people/manuel-le-gallo-bourdeau), and [Kaoutar El Maghraoui](https://www.linkedin.com/in/kaoutar-el-maghraoui/)\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/IBM/aihwkit/blob/master/notebooks/tutorial/hw_aware_training.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\"/>\n",
    "</a>\n",
    "\n",
    "The IBM Analog Hardware Acceleration Kit (AIHWKIT) is an open-source Python toolkit for exploring and using the capabilities of in-memory computing devices (PCM, RRAM and others) in the context of artificial intelligence. The PyTorch integration consists of a series of primitives and features that allow using the toolkit within PyTorch.\n",
    "The GitHub repository can be found at: https://github.com/IBM/aihwkit\n",
    "To learn more about Analog AI and the harware befind it, refer to this webpage: https://aihw-composer.draco.res.ibm.com/about\n",
    "\n",
    "##Installing AIHWKIT\n",
    "\n",
    "The first thing to do is install AIHWKIT and dependencies in your environment. The preferred way to install this package is by using the Python package index (please uncomment this line to install in your environment if not previously installed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEfolvgr-HSO",
    "outputId": "929673f8-7bf4-45f8-94f5-4ac82f97e82b"
   },
   "outputs": [],
   "source": [
    "# To install the cpu-only enabled kit, un-comment the line below\n",
    "#!pip install aihwkit\n",
    "\n",
    "# To install the GPU-enabled wheel, use the commands below.\n",
    "!wget https://aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud/aihwkit-0.8.0+cuda117-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install aihwkit-0.8.0+cuda117-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05XKOcQdoWis"
   },
   "source": [
    "Upload the utils directory in your runtime environment if you are using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "8dRBAFI2xcEK",
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# various utility functions\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class LambdaLayer(torch.nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option=\"A\"):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = torch.nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = torch.nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == \"A\":\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(\n",
    "                        x[:, :, ::2, ::2],\n",
    "                        (0, 0, 0, 0, planes // 4, planes // 4),\n",
    "                        \"constant\",\n",
    "                        0,\n",
    "                    )\n",
    "                )\n",
    "            elif option == \"B\":\n",
    "                self.shortcut = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_planes,\n",
    "                        self.expansion * planes,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                    torch.nn.BatchNorm2d(self.expansion * planes),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self, block, num_blocks, n_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            3, 16, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = torch.nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = torch.nn.Linear(64, n_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet32(n_classes=10):\n",
    "    return ResNet(BasicBlock, [5, 5, 5], n_classes=n_classes)\n",
    "\n",
    "\n",
    "class TorchCutout(object):\n",
    "    def __init__(self, length, fill=(0.0, 0.0, 0.0)):\n",
    "        self.length = length\n",
    "        self.fill = torch.tensor(fill).reshape(shape=(3, 1, 1))\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "        y1 = np.clip(y - self.length // 2, 0, h)\n",
    "        y2 = np.clip(y + self.length // 2, 0, h)\n",
    "        x1 = np.clip(x - self.length // 2, 0, w)\n",
    "        x2 = np.clip(x + self.length // 2, 0, w)\n",
    "        img[:, y1:y2, x1:x2] = self.fill\n",
    "        return img\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "def load_cifar10(batch_size, path):\n",
    "    transform_train = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.RandomCrop(32, padding=4),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "            ),\n",
    "            TorchCutout(length=8),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    transform_test = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=path, train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=path, train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=1\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=1\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def load_cifar10_ffcv(batch_size, path):\n",
    "    # - FFCV specific imports\n",
    "    from ffcv.fields import IntField, RGBImageField\n",
    "    from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "    from ffcv.loader import Loader, OrderOption\n",
    "    from ffcv.pipeline.operation import Operation\n",
    "    from ffcv.transforms import (\n",
    "        RandomHorizontalFlip,\n",
    "        Cutout,\n",
    "        RandomTranslate,\n",
    "        Convert,\n",
    "        ToDevice,\n",
    "        ToTensor,\n",
    "        ToTorchImage,\n",
    "    )\n",
    "    from ffcv.transforms.common import Squeeze\n",
    "    from ffcv.writer import DatasetWriter\n",
    "\n",
    "    datasets = {\n",
    "        \"train\": torchvision.datasets.CIFAR10(path, train=True, download=True),\n",
    "        \"test\": torchvision.datasets.CIFAR10(path, train=False, download=True),\n",
    "    }\n",
    "\n",
    "    for name, ds in datasets.items():\n",
    "        writer = DatasetWriter(\n",
    "            os.path.join(path, f\"cifar_{name}.beton\"),\n",
    "            {\"image\": RGBImageField(), \"label\": IntField()},\n",
    "        )\n",
    "        writer.from_indexed_dataset(ds)\n",
    "\n",
    "    # Note that statistics are wrt to uin8 range, [0,255].\n",
    "    CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "    CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "\n",
    "    loaders = {}\n",
    "    for name in [\"train\", \"test\"]:\n",
    "        label_pipeline: List[Operation] = [\n",
    "            IntDecoder(),\n",
    "            ToTensor(),\n",
    "            ToDevice(device),\n",
    "            Squeeze(),\n",
    "        ]\n",
    "        image_pipeline: List[Operation] = [SimpleRGBImageDecoder()]\n",
    "\n",
    "        # Add image transforms and normalization\n",
    "        if name == \"train\":\n",
    "            image_pipeline.extend(\n",
    "                [\n",
    "                    RandomTranslate(padding=4),\n",
    "                    RandomHorizontalFlip(),\n",
    "                    Cutout(\n",
    "                        8, tuple(map(int, CIFAR_MEAN))\n",
    "                    ),  # - Note Cutout is done before normalization.\n",
    "                ]\n",
    "            )\n",
    "        image_pipeline.extend(\n",
    "            [\n",
    "                ToTensor(),\n",
    "                ToDevice(device, non_blocking=True),\n",
    "                ToTorchImage(),\n",
    "                Convert(torch.float32),\n",
    "                torchvision.transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # - Create loaders\n",
    "        loaders[name] = Loader(\n",
    "            os.path.join(path, f\"cifar_{name}.beton\"),\n",
    "            batch_size=batch_size,\n",
    "            num_workers=4,\n",
    "            order=OrderOption.RANDOM,\n",
    "            drop_last=(name == \"train\"),\n",
    "            pipelines={\"image\": image_pipeline, \"label\": label_pipeline},\n",
    "        )\n",
    "\n",
    "    return loaders[\"train\"], loaders[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9m1qDEsd-C4H"
   },
   "outputs": [],
   "source": [
    "# - Generic imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# - Tutorial specific imports\n",
    "#from utils.misc import load_cifar10, load_cifar10_ffcv, device, resnet32\n",
    "#from utils.plotting import plt\n",
    "\n",
    "# - AIHWKIT related imports\n",
    "from aihwkit.nn.conversion import convert_to_analog\n",
    "from aihwkit.optim import AnalogSGD\n",
    "from aihwkit.simulator.presets.utils import IOParameters\n",
    "from aihwkit.simulator.presets import StandardHWATrainingPreset\n",
    "from aihwkit.inference.noise.pcm import PCMLikeNoiseModel\n",
    "from aihwkit.inference.compensation.drift import GlobalDriftCompensation\n",
    "from aihwkit.simulator.configs import InferenceRPUConfig\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    WeightModifierType,\n",
    "    BoundManagementType,\n",
    "    WeightClipType,\n",
    "    NoiseManagementType,\n",
    "    WeightRemapType,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJpJeStD-C4M"
   },
   "source": [
    "## RPU Config\n",
    "The RPU configuration specifies the parameters necessary for hardware-aware training. The `InferenceRPUConfig` comes with default settings, which are typically changed after initialization. Note that this determines the tile backend used. `InferenceRPUConfig` uses the CUDA implementation of the tile, while `TorchInferenceRPUConfig` uses a pure torch based approach. Using the CUDA tile is typically faster if convolutions are used because it supports an indexed implementation, while the torch-based implementation treats a convolution as a linear operation by unfolding the input, which is typically slow.\n",
    "\n",
    "### `rpu_config.modifier`\n",
    "The modifier injects noise into the weights for each batch during training. The type of the noise can be set by `rpu_config.modifier.type`. In this case, we use additive Gaussian noise, which works well. The additive Gaussian noise is applied to the weights on the tile, which are normalized to [-1,1] in our case (more about that in `rpu_config.mapping`). Since the weights are normalized, the magnitude `rpu_config.modifier.std_dev` corresponds to the percent of noise relative to the weights. In this case, we apply 6% noise.\n",
    "\n",
    "### `rpu_config.mapping`\n",
    "We can also put the bias in analog by setting `rpu_config.mapping.digital_bias`, which we don't do since the bias is implemented in digital on most hardware. The following parameters relate to how the weights are normalized to the default range [-1,1]. `rpu_config.mapping.weight_scaling_omega=1.0` initially sets the weight scale to the `abs().max()` of the weight. `WeightRemapType.LAYERWISE_SYMMETRIC` ensures that each weight gets remapped to [-1,1] after every weight update.\n",
    "\n",
    "### `rpu_config.clip`\n",
    "Tight weight distributions are important when we map the weights to conductance values. This can be ensured by specifying `rpu_config.clip.type`. In this case, we clip each weight after every batch update around `rpu_config.clip.sigma` many standard deviations of the weights. I.e. `w = w.clamp(-a*w.std(),a*w.std())`.\n",
    "\n",
    "### `rpu_config.forward`\n",
    "This part of the RPU config determines the non-idealities at a tile level, e.g., output noise, short-term weight noise, quantization, etc. In this case, we set the DAC and ADC resolution to 8 bits and the input bound to 1.0. This has the effect that the inputs are mapped to [-1,1] and quantized to 8 bits. The output bound is set to 12.0. Note that all of the non-idealities of the forward pass can be turned off by setting `rpu_config.forward.is_perfect=True`. This also has the effect of faster runtime since we can take a shortcut in the MVM computation. The bound management typically manages the input scale when there appears clipping at the ADC. We can, for example, configure it so that the input is reduced until no clipping at the ADCs occurs. We set it to None in this case. The noise management determines how the inputs are mapped to the input bound (here set to 1.0). If the noise management is set to `ABS_MAX`, we dynamically compute the `abs().max()` of the input and use this to normalize the input. If the input ranges are learned, set this to `NONE`.\n",
    "\n",
    "### `rpu_config.pre_post.input_range`\n",
    "Analog and digital accelerators typically quantize their MVM inputs by multiplying them by a scalar that maps the values to a specific range (e.g., for int8 this scale would be 127. / `input_range of input`). We can learn the input range parameter by setting `rpu_config.pre_post.input_range.enable=True`. We can calibrate the initial input range from data by setting `rpu_config.pre_post.input_range.init_from_data` to the number of batches we would like to use for this calibration. In this case, 3.0 standard deviations are taken from the batch to update the input range learning. After 50 seen batches, the input ranges are updated from gradients. Generally, the input ranges are widened after initialization. When less than `1-rpu_config.pre_post.input_range.input_min_percentage` percent of the inputs are clipped when the current input range is used, the input range is shrunk with a rate proportional to `rpu_config.pre_post.input_range.decay`. In other words, when the input range is too loose (i.e., no inputs are clipped), we shrink it again. Increasing the decay increases the speed with which we shrink it.\n",
    "\n",
    "Finally, we set the `noise_model` used for inference. This `noise_model` determines, for example, what type of programming noise is applied when we program the device. We can also simulate drift and even drift compensation. The compensation method can be set by `rpu_config.drift_compensation`. Here we use global drift compensation, which uses a reference input to determine the optimal scale that needs to be applied to the outputs in order to compensate for the drifted weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHmjiuKn-C4O"
   },
   "outputs": [],
   "source": [
    "def gen_rpu_config():\n",
    "    rpu_config = InferenceRPUConfig()\n",
    "    rpu_config.modifier.std_dev = 0.06\n",
    "    rpu_config.modifier.type = WeightModifierType.ADD_NORMAL\n",
    "\n",
    "    rpu_config.mapping.digital_bias = True\n",
    "    rpu_config.mapping.weight_scaling_omega = 1.0\n",
    "    rpu_config.mapping.weight_scaling_columnwise = False\n",
    "    rpu_config.mapping.out_scaling_columnwise = False\n",
    "    rpu_config.remap.type = WeightRemapType.LAYERWISE_SYMMETRIC\n",
    "\n",
    "    rpu_config.clip.type = WeightClipType.LAYER_GAUSSIAN\n",
    "    rpu_config.clip.sigma = 2.0\n",
    "\n",
    "    rpu_config.forward = IOParameters()\n",
    "    rpu_config.forward.is_perfect = False\n",
    "    rpu_config.forward.out_noise = 0.0\n",
    "    rpu_config.forward.inp_bound = 1.0\n",
    "    rpu_config.forward.inp_res = 1 / (2**8 - 2)\n",
    "    rpu_config.forward.out_bound = 12\n",
    "    rpu_config.forward.out_res = 1 / (2**8 - 2)\n",
    "    rpu_config.forward.bound_management = BoundManagementType.NONE\n",
    "    rpu_config.forward.noise_management = NoiseManagementType.NONE\n",
    "\n",
    "    rpu_config.pre_post.input_range.enable = True\n",
    "    rpu_config.pre_post.input_range.decay = 0.01\n",
    "    rpu_config.pre_post.input_range.init_from_data = 50\n",
    "    rpu_config.pre_post.input_range.init_std_alpha = 3.0\n",
    "    rpu_config.pre_post.input_range.input_min_percentage = 0.995\n",
    "    rpu_config.pre_post.input_range.manage_output_clipping = False\n",
    "\n",
    "    rpu_config.noise_model = PCMLikeNoiseModel(g_max=25.0)\n",
    "    rpu_config.drift_compensation = GlobalDriftCompensation()\n",
    "    return rpu_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_She6Vv-C4P"
   },
   "outputs": [],
   "source": [
    "# - Standard train and test routines\n",
    "def train_step(model, optimizer, criterion, trainloader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in trainloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return train_loss / total, 100.0 * correct / total\n",
    "\n",
    "\n",
    "def test_step(model, criterion, testloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    print(f\"Test loss {test_loss/total:.4f} test acc. {100.*correct/total:.2f}%\")\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0-82vyr-C4Q",
    "outputId": "05431c47-efa4-4ea6-e759-5e3b5b93f66b"
   },
   "outputs": [],
   "source": [
    "# - Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "import os\n",
    "# - Get the dataloader\n",
    "batch_size = 128\n",
    "trainloader, testloader = load_cifar10(\n",
    "    batch_size=batch_size, path=os.path.expanduser(\"~/Data/\")\n",
    ")\n",
    "\n",
    "# - Change to True if one of the models should be re-trained\n",
    "retrain_baseline = False\n",
    "retrain_finetuned_model = False\n",
    "\n",
    "# - Some hyperparameters\n",
    "lr = 0.05\n",
    "epochs = 200\n",
    "epochs_finetuning = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aG-rrTt5-C4R"
   },
   "outputs": [],
   "source": [
    "# - Define model, criterion, optimizer and scheduler.\n",
    "model = resnet32()\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD4Y2wot-C4R"
   },
   "source": [
    "We typically first pre-train a baseline model that we later fine-tune using noise injection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_Cvn56G-C4R",
    "outputId": "bd54f7a1-a836-4b27-b627-eb18b524dd14"
   },
   "outputs": [],
   "source": [
    "# - Pre-training of the network\n",
    "if retrain_baseline:\n",
    "    pbar = tqdm(range(epochs))\n",
    "    for epoch in pbar:\n",
    "        train_loss, train_acc = train_step(model, optimizer, criterion, trainloader)\n",
    "        pbar.set_description(f\"Epoch {epoch} Train loss: {train_loss:.4f} train acc. {train_acc:.2f}%\")\n",
    "        if epoch % 5 == 0:\n",
    "            test_step(model, criterion, testloader)\n",
    "        scheduler.step()\n",
    "    torch.save(model.state_dict(), \"Models/pre_trained_model.th\")\n",
    "else:\n",
    "    !wget -P Models/ https://aihwkit-tutorial.s3.us-east.cloud-object-storage.appdomain.cloud/pre_trained_model.th\n",
    "    model.load_state_dict(torch.load(\"Models/pre_trained_model.th\", map_location=device))\n",
    "    print(f\"Pretrained test acc. {test_step(model, criterion, testloader)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDdP2piA-C4S"
   },
   "source": [
    "We first convert our model to an analog model using `convert_to_analog` where we pass the model and the RPU config. The optimizer, in this case, `AnalogSGD`.\n",
    "The rest is standard training. Analog models can be easily saved like regular models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CF6ddwgw-C4S",
    "outputId": "17dbd555-d243-4cac-eed6-c8652d8e79bb"
   },
   "outputs": [],
   "source": [
    "# - Fine-tuning\n",
    "analog_model = convert_to_analog(model, gen_rpu_config())\n",
    "if retrain_finetuned_model:\n",
    "    optimizer = AnalogSGD(\n",
    "        analog_model.parameters(), lr=lr / 10.0, momentum=0.9, weight_decay=5e-4\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    test_accs = torch.empty(epochs_finetuning)\n",
    "    pbar = tqdm(range(epochs_finetuning))\n",
    "    for epoch in pbar:\n",
    "        train_loss, train_acc = train_step(analog_model, optimizer, criterion, trainloader)\n",
    "        pbar.set_description(f\"Epoch {epoch} Train loss: {train_loss:.4f} train acc. {train_acc:.2f}%\")\n",
    "        test_accs[epoch] = test_step(analog_model, criterion, testloader)\n",
    "        scheduler.step()\n",
    "\n",
    "    torch.save(analog_model.state_dict(), \"Models/finetuned_model.th\")\n",
    "    torch.save(test_accs, \"Models/test_accs.th\")\n",
    "\n",
    "else:\n",
    "    !wget -P Models/ https://aihwkit-tutorial.s3.us-east.cloud-object-storage.appdomain.cloud/test_accs.th\n",
    "    !wget -P Models/ https://aihwkit-tutorial.s3.us-east.cloud-object-storage.appdomain.cloud/finetuned_model.th\n",
    "    test_accs = torch.load(\"Models/test_accs.th\")\n",
    "    analog_model.load_state_dict(\n",
    "        torch.load(\"Models/finetuned_model.th\", map_location=device)\n",
    "    )\n",
    "    print(f\"Finetuned test acc. {test_step(analog_model, criterion, testloader)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hmH9LTr-C4T"
   },
   "source": [
    "Let's look at the test accuracy over time for the fine-tuned model. The initial accuracy is pretty low. This is because the weights are clipped to 2 standard deviations. This initially hurts performance before the network recovers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JRMIngM203Ph",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rcParams[\"font.size\"] = 16\n",
    "rcParams[\"axes.linewidth\"] = 1.1\n",
    "rcParams[\"axes.labelpad\"] = 3.0\n",
    "plot_color_cycle = plt.cycler(\n",
    "    \"color\",\n",
    "    [\n",
    "        \"#9b59b6\",\n",
    "        \"#3498db\",\n",
    "        \"#95a5a6\",\n",
    "        \"#e74c3c\",\n",
    "        \"#34495e\",\n",
    "        \"#2ecc71\",\n",
    "        \"#1E2460\",\n",
    "        \"#B5B8B1\",\n",
    "        \"#734222\",\n",
    "        \"#A52019\",\n",
    "    ],\n",
    ")\n",
    "rcParams[\"axes.prop_cycle\"] = plot_color_cycle\n",
    "rcParams[\"axes.xmargin\"] = 0\n",
    "rcParams[\"axes.ymargin\"] = 0\n",
    "rcParams.update(\n",
    "    {\n",
    "        \"figure.figsize\": (6.4, 4.8),\n",
    "        \"figure.subplot.left\": 0.07,\n",
    "        \"figure.subplot.right\": 0.946,\n",
    "        \"figure.subplot.bottom\": 0.1,\n",
    "        \"figure.subplot.top\": 0.965,\n",
    "        \"axes.autolimit_mode\": \"round_numbers\",\n",
    "        \"axes.grid\": True,\n",
    "        \"xtick.major.size\": 7,\n",
    "        \"xtick.minor.size\": 3.5,\n",
    "        \"xtick.major.width\": 1.1,\n",
    "        \"xtick.minor.width\": 1.1,\n",
    "        \"xtick.major.pad\": 5,\n",
    "        \"xtick.minor.visible\": True,\n",
    "        \"ytick.major.size\": 7,\n",
    "        \"ytick.minor.size\": 3.5,\n",
    "        \"ytick.major.width\": 1.1,\n",
    "        \"ytick.minor.width\": 1.1,\n",
    "        \"ytick.major.pad\": 5,\n",
    "        \"ytick.minor.visible\": True,\n",
    "        \"lines.markersize\": 10,\n",
    "        \"lines.markerfacecolor\": \"none\",\n",
    "        \"lines.markeredgewidth\": 0.8,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "Lxm4YHJz-C4T",
    "outputId": "4037e66a-aeb9-4a3b-e691-9d41cc98f9ad"
   },
   "outputs": [],
   "source": [
    "plt.title(\"Finetunig test accuracy\")\n",
    "plt.plot(test_accs, marker=\"d\", linestyle=\"--\", color=\"b\")\n",
    "plt.ylabel(\"Test acc. (%)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqTcidHy-C4U"
   },
   "source": [
    "We can also verify that the weights are clipped by looking at one random weight matrix in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "RMHdErKQ-C4U",
    "outputId": "5634dc2f-15bb-407c-f9e9-757497359c71"
   },
   "outputs": [],
   "source": [
    "w, _ = (\n",
    "    analog_model\n",
    "    .module\n",
    "    .layer3[0]\n",
    "    .conv1.analog_module.get_weights(apply_weight_scaling=True)\n",
    ")\n",
    "plt.hist(w.flatten().detach().numpy(), color=\"r\", bins=50)\n",
    "plt.xlabel(\"Unnormalized weight\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrLKLZNb-C4V"
   },
   "source": [
    "Finally, we would like to see how robust our model is. We first have to convert our pre-trained model to analog.\n",
    "We then repeatedly call `drift_analog_weights` with a time value (in seconds). This simulates the drifting of the weights to the specified time. Note that this call also programs the weights, i.e. it simulates programming by applying specific programming noise. This noise model is defined in the `noise_model` of the RPU config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uAF1fy68-C4V",
    "outputId": "c5e5762b-cb7b-4dba-8ff9-a33bdd885468"
   },
   "outputs": [],
   "source": [
    "converted_model = convert_to_analog(model, StandardHWATrainingPreset())\n",
    "# - For programming the model, we need to put it into eval() mode\n",
    "converted_model = converted_model.eval()\n",
    "analog_model = analog_model.eval()\n",
    "# - We repeat each measurement 5 times\n",
    "n_rep = 5\n",
    "t_inferences = [60., 3600., 86400., 2592000., 31104000.]\n",
    "drifted_test_accs = torch.zeros(size=(len(t_inferences),n_rep))\n",
    "drifted_test_accs_baseline = torch.zeros(size=(len(t_inferences),n_rep))\n",
    "for i,t in enumerate(t_inferences):\n",
    "    for j in range(n_rep):\n",
    "        converted_model.drift_analog_weights(t)\n",
    "        drifted_test_accs_baseline[i,j] = test_step(converted_model, criterion, testloader)\n",
    "        analog_model.drift_analog_weights(t)\n",
    "        drifted_test_accs[i,j] = test_step(analog_model, criterion, testloader)\n",
    "plt.errorbar(t_inferences, drifted_test_accs.mean(1), drifted_test_accs.std(1), capsize=3, label=\"HW-aware\")\n",
    "plt.errorbar(t_inferences, drifted_test_accs_baseline.mean(1), drifted_test_accs_baseline.std(1), capsize=3, label=\"Baseline\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"Test acc. (%)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TXmcDuN-C4W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "8552d10987f98ef8320154686d1598142476aa8dfb1019e7b5164f51d5b1d29f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
