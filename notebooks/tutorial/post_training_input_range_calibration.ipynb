{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7revaZCcOzNv"
   },
   "source": [
    "# IBM Analog Hardware Acceleration Kit (AIHWKIT): Post Training Input Range Calibration\n",
    "\n",
    "### Authors: [Julian Büchel](https://www.linkedin.com/in/julian-büchel-0673991a3/), [Manuel Le Gallo-Bourdeau](https://research.ibm.com/people/manuel-le-gallo-bourdeau), and [Kaoutar El Maghraoui](https://www.linkedin.com/in/kaoutar-el-maghraoui/)\n",
    "\n",
    "In this notebook, we show how you can use the aihwkit library to perform post-training input range calibration for a pre-trained Analog model to improve its inference accuracy.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/IBM/aihwkit/blob/master/notebooks/tutorial/post_training_input_range_calibration.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\"/>\n",
    "</a>\n",
    "\n",
    "The IBM Analog Hardware Acceleration Kit (AIHWKIT) is an open-source Python toolkit for exploring and using the capabilities of in-memory computing devices (PCM, RRAM and others) in the context of artificial intelligence. The PyTorch integration consists of a series of primitives and features that allow using the toolkit within PyTorch.\n",
    "The GitHub repository can be found at: https://github.com/IBM/aihwkit\n",
    "To learn more about Analog AI and the harware befind it, refer to this webpage: https://aihw-composer.draco.res.ibm.com/about\n",
    "\n",
    "### Installing AIHWKIT\n",
    "\n",
    "The first thing to do is install AIHWKIT and dependencies in your environment. The preferred way to install this package is by using the Python package index (please uncomment this line to install in your environment if not previously installed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dN6o8fsqO89I",
    "outputId": "2a5853ad-7b33-4826-d8a6-fa2e5f662e6c"
   },
   "outputs": [],
   "source": [
    "# To install the cpu-only enabled kit, un-comment the line below\n",
    "# %pip install aihwkit\n",
    "\n",
    "# To install the GPU-enabled wheel go to https://aihwkit.readthedocs.io/en/latest/advanced_install.html#install-the-aihwkit-using-pip\n",
    "# and copy the option on GPU options that best suits your enviroment and paste it below and run the cell. For example, Python 3.10 and CUDA 12.1:\n",
    "# !wget https://aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud/aihwkit-0.9.2+cuda121-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "# %pip install aihwkit-0.9.2+cuda121-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZCLK6Nvx4hbE",
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# various utility functions\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class LambdaLayer(torch.nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option=\"A\"):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = torch.nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = torch.nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == \"A\":\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(\n",
    "                        x[:, :, ::2, ::2],\n",
    "                        (0, 0, 0, 0, planes // 4, planes // 4),\n",
    "                        \"constant\",\n",
    "                        0,\n",
    "                    )\n",
    "                )\n",
    "            elif option == \"B\":\n",
    "                self.shortcut = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_planes,\n",
    "                        self.expansion * planes,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                    torch.nn.BatchNorm2d(self.expansion * planes),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self, block, num_blocks, n_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            3, 16, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = torch.nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = torch.nn.Linear(64, n_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet32(n_classes=10):\n",
    "    return ResNet(BasicBlock, [5, 5, 5], n_classes=n_classes)\n",
    "\n",
    "class TorchCutout(object):\n",
    "    def __init__(self, length, fill=(0.0, 0.0, 0.0)):\n",
    "        self.length = length\n",
    "        self.fill = torch.tensor(fill).reshape(shape=(3, 1, 1))\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "        y1 = np.clip(y - self.length // 2, 0, h)\n",
    "        y2 = np.clip(y + self.length // 2, 0, h)\n",
    "        x1 = np.clip(x - self.length // 2, 0, w)\n",
    "        x2 = np.clip(x + self.length // 2, 0, w)\n",
    "        img[:, y1:y2, x1:x2] = self.fill\n",
    "        return img\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "def load_cifar10(batch_size, path):\n",
    "    transform_train = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.RandomCrop(32, padding=4),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "            ),\n",
    "            TorchCutout(length=8),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    transform_test = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=path, train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=path, train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=1\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=1\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def load_cifar10_ffcv(batch_size, path):\n",
    "    # - FFCV specific imports\n",
    "    from ffcv.fields import IntField, RGBImageField\n",
    "    from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "    from ffcv.loader import Loader, OrderOption\n",
    "    from ffcv.pipeline.operation import Operation\n",
    "    from ffcv.transforms import (\n",
    "        RandomHorizontalFlip,\n",
    "        Cutout,\n",
    "        RandomTranslate,\n",
    "        Convert,\n",
    "        ToDevice,\n",
    "        ToTensor,\n",
    "        ToTorchImage,\n",
    "    )\n",
    "    from ffcv.transforms.common import Squeeze\n",
    "    from ffcv.writer import DatasetWriter\n",
    "\n",
    "    datasets = {\n",
    "        \"train\": torchvision.datasets.CIFAR10(path, train=True, download=True),\n",
    "        \"test\": torchvision.datasets.CIFAR10(path, train=False, download=True),\n",
    "    }\n",
    "\n",
    "    for name, ds in datasets.items():\n",
    "        writer = DatasetWriter(\n",
    "            os.path.join(path, f\"cifar_{name}.beton\"),\n",
    "            {\"image\": RGBImageField(), \"label\": IntField()},\n",
    "        )\n",
    "        writer.from_indexed_dataset(ds)\n",
    "\n",
    "    # Note that statistics are wrt to uin8 range, [0,255].\n",
    "    CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "    CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "\n",
    "    loaders = {}\n",
    "    for name in [\"train\", \"test\"]:\n",
    "        label_pipeline: List[Operation] = [\n",
    "            IntDecoder(),\n",
    "            ToTensor(),\n",
    "            ToDevice(device),\n",
    "            Squeeze(),\n",
    "        ]\n",
    "        image_pipeline: List[Operation] = [SimpleRGBImageDecoder()]\n",
    "\n",
    "        # Add image transforms and normalization\n",
    "        if name == \"train\":\n",
    "            image_pipeline.extend(\n",
    "                [\n",
    "                    RandomTranslate(padding=4),\n",
    "                    RandomHorizontalFlip(),\n",
    "                    Cutout(\n",
    "                        8, tuple(map(int, CIFAR_MEAN))\n",
    "                    ),  # - Note Cutout is done before normalization.\n",
    "                ]\n",
    "            )\n",
    "        image_pipeline.extend(\n",
    "            [\n",
    "                ToTensor(),\n",
    "                ToDevice(device, non_blocking=True),\n",
    "                ToTorchImage(),\n",
    "                Convert(torch.float32),\n",
    "                torchvision.transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # - Create loaders\n",
    "        loaders[name] = Loader(\n",
    "            os.path.join(path, f\"cifar_{name}.beton\"),\n",
    "            batch_size=batch_size,\n",
    "            num_workers=4,\n",
    "            order=OrderOption.RANDOM,\n",
    "            drop_last=(name == \"train\"),\n",
    "            pipelines={\"image\": image_pipeline, \"label\": label_pipeline},\n",
    "        )\n",
    "\n",
    "    return loaders[\"train\"], loaders[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "la7BcuqIPW5Q"
   },
   "source": [
    "Import the necessary modules to perform the Post-training calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ixYTr4eCN1vJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Generic imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# - Tutorial specific imports\n",
    "#from utils.misc import load_cifar10, device, resnet32\n",
    "#from utils.plotting import plt\n",
    "\n",
    "# - AIHWKIT related imports\n",
    "from aihwkit.nn.conversion import convert_to_analog\n",
    "from aihwkit.simulator.presets.utils import IOParameters\n",
    "from aihwkit.inference.noise.pcm import PCMLikeNoiseModel\n",
    "from aihwkit.inference.compensation.drift import GlobalDriftCompensation\n",
    "from aihwkit.simulator.configs import InferenceRPUConfig\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    WeightModifierType,\n",
    "    BoundManagementType,\n",
    "    WeightClipType,\n",
    "    NoiseManagementType,\n",
    "    WeightRemapType,\n",
    ")\n",
    "from aihwkit.inference.calibration import calibrate_input_ranges, InputRangeCalibrationType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-V8V3EjPf_k"
   },
   "source": [
    "Define the RPU configuration that abstracts all the hardware related parameters and noise characteristics of the simulated Analog hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qRicCGYqN1vO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_rpu_config():\n",
    "    rpu_config = InferenceRPUConfig()\n",
    "    rpu_config.modifier.std_dev = 0.06\n",
    "    rpu_config.modifier.type = WeightModifierType.ADD_NORMAL\n",
    "\n",
    "    rpu_config.mapping.digital_bias = True\n",
    "    rpu_config.mapping.weight_scaling_omega = 1.0\n",
    "    rpu_config.mapping.weight_scaling_columnwise = False\n",
    "    rpu_config.mapping.out_scaling_columnwise = False\n",
    "    rpu_config.remap.type = WeightRemapType.LAYERWISE_SYMMETRIC\n",
    "\n",
    "    rpu_config.clip.type = WeightClipType.LAYER_GAUSSIAN\n",
    "    rpu_config.clip.sigma = 2.0\n",
    "\n",
    "    rpu_config.forward = IOParameters()\n",
    "    rpu_config.forward.is_perfect = False\n",
    "    rpu_config.forward.out_noise = 0.0\n",
    "    rpu_config.forward.inp_bound = 1.0\n",
    "    rpu_config.forward.inp_res = 1 / (2**8 - 2)\n",
    "    rpu_config.forward.out_bound = 12\n",
    "    rpu_config.forward.out_res = 1 / (2**8 - 2)\n",
    "    rpu_config.forward.bound_management = BoundManagementType.NONE\n",
    "    rpu_config.forward.noise_management = NoiseManagementType.NONE\n",
    "\n",
    "    rpu_config.pre_post.input_range.enable = True\n",
    "    rpu_config.pre_post.input_range.decay = 0.01\n",
    "    rpu_config.pre_post.input_range.init_from_data = 50\n",
    "    rpu_config.pre_post.input_range.init_std_alpha = 3.0\n",
    "    rpu_config.pre_post.input_range.input_min_percentage = 0.995\n",
    "    rpu_config.pre_post.input_range.manage_output_clipping = False\n",
    "\n",
    "    rpu_config.noise_model = PCMLikeNoiseModel(g_max=25.0)\n",
    "    rpu_config.drift_compensation = GlobalDriftCompensation()\n",
    "    return rpu_config\n",
    "\n",
    "\n",
    "def test_step(model, criterion, testloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    print(f\"Test loss {test_loss/total:.4f} test acc. {100.*correct/total:.2f}%\")\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foWVAgW0i2MT"
   },
   "source": [
    "Load the datase, define the ResNet32 model, optimizer and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJzTcRkRN1vP",
    "outputId": "dca858f0-112e-4a03-9b85-18176fc0e63a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# - Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# - Get the dataloader\n",
    "batch_size = 128\n",
    "trainloader, testloader = load_cifar10(\n",
    "    batch_size=batch_size, path=os.path.expanduser(\"~/Data\")\n",
    ")\n",
    "\n",
    "# - Define model, criterion, optimizer and scheduler.\n",
    "model = resnet32()\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfGUyS1BN1vQ",
    "outputId": "a3250fda-89f9-4f29-af1b-eca17170dba1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# - Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# - Get the dataloader\n",
    "batch_size = 128\n",
    "trainloader, testloader = load_cifar10(\n",
    "    batch_size=batch_size, path=os.path.expanduser(\"~/Data/\")\n",
    ")\n",
    "\n",
    "# - Change to True if one of the models should be re-trained\n",
    "retrain_baseline = False\n",
    "retrain_finetuned_model = False\n",
    "\n",
    "# - Some hyperparameters\n",
    "lr = 0.05\n",
    "epochs = 200\n",
    "epochs_finetuning = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuXtxjjrjEF0"
   },
   "source": [
    "Convert the ResNet model to its Analog version. Initialize the analog model withpre-trained and fine-tuned model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZ0mkeqXN1vQ",
    "outputId": "1ea1cc0a-d7ae-4afc-e076-7f7a3e8383b5",
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AnalogWrapperResNet:\n\tMissing key(s) in state_dict: \"conv1.analog_module.input_range_update_idx\", \"layer1.0.conv1.analog_module.input_range_update_idx\", \"layer1.0.conv2.analog_module.input_range_update_idx\", \"layer1.1.conv1.analog_module.input_range_update_idx\", \"layer1.1.conv2.analog_module.input_range_update_idx\", \"layer1.2.conv1.analog_module.input_range_update_idx\", \"layer1.2.conv2.analog_module.input_range_update_idx\", \"layer1.3.conv1.analog_module.input_range_update_idx\", \"layer1.3.conv2.analog_module.input_range_update_idx\", \"layer1.4.conv1.analog_module.input_range_update_idx\", \"layer1.4.conv2.analog_module.input_range_update_idx\", \"layer2.0.conv1.analog_module.input_range_update_idx\", \"layer2.0.conv2.analog_module.input_range_update_idx\", \"layer2.1.conv1.analog_module.input_range_update_idx\", \"layer2.1.conv2.analog_module.input_range_update_idx\", \"layer2.2.conv1.analog_module.input_range_update_idx\", \"layer2.2.conv2.analog_module.input_range_update_idx\", \"layer2.3.conv1.analog_module.input_range_update_idx\", \"layer2.3.conv2.analog_module.input_range_update_idx\", \"layer2.4.conv1.analog_module.input_range_update_idx\", \"layer2.4.conv2.analog_module.input_range_update_idx\", \"layer3.0.conv1.analog_module.input_range_update_idx\", \"layer3.0.conv2.analog_module.input_range_update_idx\", \"layer3.1.conv1.analog_module.input_range_update_idx\", \"layer3.1.conv2.analog_module.input_range_update_idx\", \"layer3.2.conv1.analog_module.input_range_update_idx\", \"layer3.2.conv2.analog_module.input_range_update_idx\", \"layer3.3.conv1.analog_module.input_range_update_idx\", \"layer3.3.conv2.analog_module.input_range_update_idx\", \"layer3.4.conv1.analog_module.input_range_update_idx\", \"layer3.4.conv2.analog_module.input_range_update_idx\", \"linear.analog_module.input_range_update_idx\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels/finetuned_model.th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m---> 13\u001b[0m \u001b[43manalog_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModels/finetuned_model.th\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tile_name, tile \u001b[38;5;129;01min\u001b[39;00m analog_model\u001b[38;5;241m.\u001b[39mnamed_analog_tiles():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input range \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtile\u001b[38;5;241m.\u001b[39minput_range\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/update-notebooks/lib/python3.10/site-packages/aihwkit/nn/modules/base.py:249\u001b[0m, in \u001b[0;36mAnalogLayerBase.load_state_dict\u001b[0;34m(self, state_dict, strict, load_rpu_config, strict_rpu_config_check)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m analog_tile \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalog_tiles():\n\u001b[1;32m    248\u001b[0m     analog_tile\u001b[38;5;241m.\u001b[39mset_load_rpu_config_state(load_rpu_config, strict_rpu_config_check)\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/update-notebooks/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AnalogWrapperResNet:\n\tMissing key(s) in state_dict: \"conv1.analog_module.input_range_update_idx\", \"layer1.0.conv1.analog_module.input_range_update_idx\", \"layer1.0.conv2.analog_module.input_range_update_idx\", \"layer1.1.conv1.analog_module.input_range_update_idx\", \"layer1.1.conv2.analog_module.input_range_update_idx\", \"layer1.2.conv1.analog_module.input_range_update_idx\", \"layer1.2.conv2.analog_module.input_range_update_idx\", \"layer1.3.conv1.analog_module.input_range_update_idx\", \"layer1.3.conv2.analog_module.input_range_update_idx\", \"layer1.4.conv1.analog_module.input_range_update_idx\", \"layer1.4.conv2.analog_module.input_range_update_idx\", \"layer2.0.conv1.analog_module.input_range_update_idx\", \"layer2.0.conv2.analog_module.input_range_update_idx\", \"layer2.1.conv1.analog_module.input_range_update_idx\", \"layer2.1.conv2.analog_module.input_range_update_idx\", \"layer2.2.conv1.analog_module.input_range_update_idx\", \"layer2.2.conv2.analog_module.input_range_update_idx\", \"layer2.3.conv1.analog_module.input_range_update_idx\", \"layer2.3.conv2.analog_module.input_range_update_idx\", \"layer2.4.conv1.analog_module.input_range_update_idx\", \"layer2.4.conv2.analog_module.input_range_update_idx\", \"layer3.0.conv1.analog_module.input_range_update_idx\", \"layer3.0.conv2.analog_module.input_range_update_idx\", \"layer3.1.conv1.analog_module.input_range_update_idx\", \"layer3.1.conv2.analog_module.input_range_update_idx\", \"layer3.2.conv1.analog_module.input_range_update_idx\", \"layer3.2.conv2.analog_module.input_range_update_idx\", \"layer3.3.conv1.analog_module.input_range_update_idx\", \"layer3.3.conv2.analog_module.input_range_update_idx\", \"layer3.4.conv1.analog_module.input_range_update_idx\", \"layer3.4.conv2.analog_module.input_range_update_idx\", \"linear.analog_module.input_range_update_idx\". "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "if not os.path.exists(\"Models\"):\n",
    "    os.makedirs(\"Models\")\n",
    "\n",
    "analog_model = convert_to_analog(model, gen_rpu_config())\n",
    "\n",
    "url = \"https://aihwkit-tutorial.s3.us-east.cloud-object-storage.appdomain.cloud/finetuned_model.th\"\n",
    "response = requests.get(url)\n",
    "with open(\"Models/finetuned_model.th\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "analog_model.load_state_dict(\n",
    "    torch.load(\"Models/finetuned_model.th\", map_location=device)\n",
    ")\n",
    "for tile_name, tile in analog_model.named_analog_tiles():\n",
    "    print(f\"{tile_name} input range {tile.input_range.data.item():.4f}\")\n",
    "print(f\"Test acc. (learned input ranges) {test_step(analog_model, criterion, testloader)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPERxGZ2N1vR"
   },
   "source": [
    "The calibration routine expects an iterator that feeds single elements that the network expects as inputs. These elements can be anything (e.g., tensors or dictionaries) as long as the model can process them.\n",
    "The next class demonstrates a simple `Sampler` that takes as input another iterator (torch dataloader) and yields only the images. The iterator can be halted by raising the `StopIteration` exception, which is used in this case to limit the number of presented batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LxEJ1nV_N1vS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \"\"\"Example of a sampler used for calibration.\"\"\"\n",
    "\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.idx = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        x, _ = next(self.loader)\n",
    "        self.idx += 1\n",
    "        if self.idx > 10:\n",
    "            raise StopIteration\n",
    "        return x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2Qr76B5N1vT"
   },
   "source": [
    "We will now sweep the quantiles and look at the downstream performance for each quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9CIiij5XN1vT",
    "outputId": "db59a171-6a41-4309-b9a2-788d1d975535",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m quantile \u001b[38;5;129;01min\u001b[39;00m quantiles:\n\u001b[1;32m      6\u001b[0m     sampler \u001b[38;5;241m=\u001b[39m Sampler(trainloader)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mcalibrate_input_ranges\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalog_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalibration_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInputRangeCalibrationType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCACHE_QUANTILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test_step(analog_model, criterion, testloader)\n\u001b[1;32m     15\u001b[0m     input_ranges \u001b[38;5;241m=\u001b[39m [tile\u001b[38;5;241m.\u001b[39minput_range\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m tile \u001b[38;5;129;01min\u001b[39;00m analog_model\u001b[38;5;241m.\u001b[39manalog_tiles()]\n",
      "File \u001b[0;32m~/miniconda3/envs/update-notebooks/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/update-notebooks/lib/python3.10/site-packages/aihwkit/inference/calibration/calibration.py:237\u001b[0m, in \u001b[0;36mcalibrate_input_ranges\u001b[0;34m(model, calibration_type, dataloader, quantile, max_samples, std_alpha, force_all_layers, verbose)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Pass through the samples\u001b[39;00m\n\u001b[1;32m    236\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m args, kwargs \u001b[38;5;129;01min\u001b[39;00m progress_bar(dataloader):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     model(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Remove hooks\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "quantiles = [0.99995, 0.9995, 0.995, 0.95]\n",
    "for quantile in quantiles:\n",
    "    sampler = Sampler(trainloader)\n",
    "    calibrate_input_ranges(\n",
    "        model=analog_model,\n",
    "        calibration_type=InputRangeCalibrationType.CACHE_QUANTILE,\n",
    "        dataloader=sampler,\n",
    "        quantile=quantile,\n",
    "        max_samples=1000\n",
    "    )\n",
    "    test_acc = test_step(analog_model, criterion, testloader)\n",
    "    input_ranges = [tile.input_range.data.item() for tile in analog_model.analog_tiles()]\n",
    "    plt.plot(input_ranges, label=f\"Quantile {quantile:.5f} acc. {test_acc:.2f}\")\n",
    "\n",
    "plt.xlabel(\"Analog tile index\")\n",
    "plt.ylabel(\"Input range\")\n",
    "plt.legend()\n",
    "plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMBELmbgN1vT"
   },
   "source": [
    "As you can see from the above graph, the best quantile is 0.9995, as it yields the best accuracy. For more modes of calibrating the input ranges, check out `aihwkit.simulator.parameters.enums.InputRangeCalibrationType`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "update-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
