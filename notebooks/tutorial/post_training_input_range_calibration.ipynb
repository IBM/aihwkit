{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7revaZCcOzNv"
   },
   "source": [
    "# IBM Analog Hardware Acceleration Kit (AIHWKIT): Post Training Input Range Calibration\n",
    "\n",
    "### Authors: [Julian Büchel](https://www.linkedin.com/in/julian-büchel-0673991a3/), [Manuel Le Gallo-Bourdeau](https://research.ibm.com/people/manuel-le-gallo-bourdeau), and [Kaoutar El Maghraoui](https://www.linkedin.com/in/kaoutar-el-maghraoui/)\n",
    "\n",
    "In this notebook, we show how you can use the aihwkit library to perform post-training input range calibration for a pre-trained Analog model to improve its inference accuracy.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/IBM/aihwkit/blob/master/notebooks/tutorial/post_training_input_range_calibration.ipynb\" target=\"_parent\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\"/>\n",
    "</a>\n",
    "\n",
    "The IBM Analog Hardware Acceleration Kit (AIHWKIT) is an open-source Python toolkit for exploring and using the capabilities of in-memory computing devices (PCM, RRAM and others) in the context of artificial intelligence. The PyTorch integration consists of a series of primitives and features that allow using the toolkit within PyTorch.\n",
    "The GitHub repository can be found at: https://github.com/IBM/aihwkit\n",
    "To learn more about Analog AI and the harware befind it, refer to this webpage: https://aihw-composer.draco.res.ibm.com/about\n",
    "\n",
    "##Installing AIHWKIT\n",
    "\n",
    "The first thing to do is install AIHWKIT and dependencies in your environment. The preferred way to install this package is by using the Python package index (please uncomment this line to install in your environment if not previously installed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dN6o8fsqO89I",
    "outputId": "2a5853ad-7b33-4826-d8a6-fa2e5f662e6c"
   },
   "outputs": [],
   "source": [
    "# To install the cpu-only enabled kit, un-comment the line below\n",
    "#!pip install aihwkit\n",
    "\n",
    "# To install the GPU-enabled wheel, use the commands below.\n",
    "!wget https://aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud/aihwkit-0.8.0+cuda117-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install aihwkit-0.8.0+cuda117-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCLK6Nvx4hbE",
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# various utility functions\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "\n",
    "class LambdaLayer(torch.nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option=\"A\"):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = torch.nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = torch.nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == \"A\":\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(\n",
    "                        x[:, :, ::2, ::2],\n",
    "                        (0, 0, 0, 0, planes // 4, planes // 4),\n",
    "                        \"constant\",\n",
    "                        0,\n",
    "                    )\n",
    "                )\n",
    "            elif option == \"B\":\n",
    "                self.shortcut = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_planes,\n",
    "                        self.expansion * planes,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                    torch.nn.BatchNorm2d(self.expansion * planes),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self, block, num_blocks, n_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            3, 16, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = torch.nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = torch.nn.Linear(64, n_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet32(n_classes=10):\n",
    "    return ResNet(BasicBlock, [5, 5, 5], n_classes=n_classes)\n",
    "\n",
    "class TorchCutout(object):\n",
    "    def __init__(self, length, fill=(0.0, 0.0, 0.0)):\n",
    "        self.length = length\n",
    "        self.fill = torch.tensor(fill).reshape(shape=(3, 1, 1))\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h = img.size(1)\n",
    "        w = img.size(2)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "        y1 = np.clip(y - self.length // 2, 0, h)\n",
    "        y2 = np.clip(y + self.length // 2, 0, h)\n",
    "        x1 = np.clip(x - self.length // 2, 0, w)\n",
    "        x2 = np.clip(x + self.length // 2, 0, w)\n",
    "        img[:, y1:y2, x1:x2] = self.fill\n",
    "        return img\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "def load_cifar10(batch_size, path):\n",
    "    transform_train = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.RandomCrop(32, padding=4),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "            ),\n",
    "            TorchCutout(length=8),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    transform_test = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=path, train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=path, train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=1\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=1\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def load_cifar10_ffcv(batch_size, path):\n",
    "    # - FFCV specific imports\n",
    "    from ffcv.fields import IntField, RGBImageField\n",
    "    from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "    from ffcv.loader import Loader, OrderOption\n",
    "    from ffcv.pipeline.operation import Operation\n",
    "    from ffcv.transforms import (\n",
    "        RandomHorizontalFlip,\n",
    "        Cutout,\n",
    "        RandomTranslate,\n",
    "        Convert,\n",
    "        ToDevice,\n",
    "        ToTensor,\n",
    "        ToTorchImage,\n",
    "    )\n",
    "    from ffcv.transforms.common import Squeeze\n",
    "    from ffcv.writer import DatasetWriter\n",
    "\n",
    "    datasets = {\n",
    "        \"train\": torchvision.datasets.CIFAR10(path, train=True, download=True),\n",
    "        \"test\": torchvision.datasets.CIFAR10(path, train=False, download=True),\n",
    "    }\n",
    "\n",
    "    for name, ds in datasets.items():\n",
    "        writer = DatasetWriter(\n",
    "            os.path.join(path, f\"cifar_{name}.beton\"),\n",
    "            {\"image\": RGBImageField(), \"label\": IntField()},\n",
    "        )\n",
    "        writer.from_indexed_dataset(ds)\n",
    "\n",
    "    # Note that statistics are wrt to uin8 range, [0,255].\n",
    "    CIFAR_MEAN = [125.307, 122.961, 113.8575]\n",
    "    CIFAR_STD = [51.5865, 50.847, 51.255]\n",
    "\n",
    "    loaders = {}\n",
    "    for name in [\"train\", \"test\"]:\n",
    "        label_pipeline: List[Operation] = [\n",
    "            IntDecoder(),\n",
    "            ToTensor(),\n",
    "            ToDevice(device),\n",
    "            Squeeze(),\n",
    "        ]\n",
    "        image_pipeline: List[Operation] = [SimpleRGBImageDecoder()]\n",
    "\n",
    "        # Add image transforms and normalization\n",
    "        if name == \"train\":\n",
    "            image_pipeline.extend(\n",
    "                [\n",
    "                    RandomTranslate(padding=4),\n",
    "                    RandomHorizontalFlip(),\n",
    "                    Cutout(\n",
    "                        8, tuple(map(int, CIFAR_MEAN))\n",
    "                    ),  # - Note Cutout is done before normalization.\n",
    "                ]\n",
    "            )\n",
    "        image_pipeline.extend(\n",
    "            [\n",
    "                ToTensor(),\n",
    "                ToDevice(device, non_blocking=True),\n",
    "                ToTorchImage(),\n",
    "                Convert(torch.float32),\n",
    "                torchvision.transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # - Create loaders\n",
    "        loaders[name] = Loader(\n",
    "            os.path.join(path, f\"cifar_{name}.beton\"),\n",
    "            batch_size=batch_size,\n",
    "            num_workers=4,\n",
    "            order=OrderOption.RANDOM,\n",
    "            drop_last=(name == \"train\"),\n",
    "            pipelines={\"image\": image_pipeline, \"label\": label_pipeline},\n",
    "        )\n",
    "\n",
    "    return loaders[\"train\"], loaders[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "la7BcuqIPW5Q"
   },
   "source": [
    "Import the necessary modules to perform the Post-training calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixYTr4eCN1vJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Generic imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# - Tutorial specific imports\n",
    "#from utils.misc import load_cifar10, device, resnet32\n",
    "#from utils.plotting import plt\n",
    "\n",
    "# - AIHWKIT related imports\n",
    "from aihwkit.nn.conversion import convert_to_analog\n",
    "from aihwkit.simulator.presets.utils import IOParameters\n",
    "from aihwkit.inference.noise.pcm import PCMLikeNoiseModel\n",
    "from aihwkit.inference.compensation.drift import GlobalDriftCompensation\n",
    "from aihwkit.simulator.configs import InferenceRPUConfig\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    WeightModifierType,\n",
    "    BoundManagementType,\n",
    "    WeightClipType,\n",
    "    NoiseManagementType,\n",
    "    WeightRemapType,\n",
    ")\n",
    "from aihwkit.inference.calibration import calibrate_input_ranges, InputRangeCalibrationType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-V8V3EjPf_k"
   },
   "source": [
    "Define the RPU configuration that abstracts all the hardware related parameters and noise characteristics of the simulated Analog hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRicCGYqN1vO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_rpu_config():\n",
    "    rpu_config = InferenceRPUConfig()\n",
    "    rpu_config.modifier.std_dev = 0.06\n",
    "    rpu_config.modifier.type = WeightModifierType.ADD_NORMAL\n",
    "\n",
    "    rpu_config.mapping.digital_bias = True\n",
    "    rpu_config.mapping.weight_scaling_omega = 1.0\n",
    "    rpu_config.mapping.weight_scaling_columnwise = False\n",
    "    rpu_config.mapping.out_scaling_columnwise = False\n",
    "    rpu_config.remap.type = WeightRemapType.LAYERWISE_SYMMETRIC\n",
    "\n",
    "    rpu_config.clip.type = WeightClipType.LAYER_GAUSSIAN\n",
    "    rpu_config.clip.sigma = 2.0\n",
    "\n",
    "    rpu_config.forward = IOParameters()\n",
    "    rpu_config.forward.is_perfect = False\n",
    "    rpu_config.forward.out_noise = 0.0\n",
    "    rpu_config.forward.inp_bound = 1.0\n",
    "    rpu_config.forward.inp_res = 1 / (2**8 - 2)\n",
    "    rpu_config.forward.out_bound = 12\n",
    "    rpu_config.forward.out_res = 1 / (2**8 - 2)\n",
    "    rpu_config.forward.bound_management = BoundManagementType.NONE\n",
    "    rpu_config.forward.noise_management = NoiseManagementType.NONE\n",
    "\n",
    "    rpu_config.pre_post.input_range.enable = True\n",
    "    rpu_config.pre_post.input_range.decay = 0.01\n",
    "    rpu_config.pre_post.input_range.init_from_data = 50\n",
    "    rpu_config.pre_post.input_range.init_std_alpha = 3.0\n",
    "    rpu_config.pre_post.input_range.input_min_percentage = 0.995\n",
    "    rpu_config.pre_post.input_range.manage_output_clipping = False\n",
    "\n",
    "    rpu_config.noise_model = PCMLikeNoiseModel(g_max=25.0)\n",
    "    rpu_config.drift_compensation = GlobalDriftCompensation()\n",
    "    return rpu_config\n",
    "\n",
    "\n",
    "def test_step(model, criterion, testloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    print(f\"Test loss {test_loss/total:.4f} test acc. {100.*correct/total:.2f}%\")\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foWVAgW0i2MT"
   },
   "source": [
    "Load the datase, define the ResNet32 model, optimizer and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJzTcRkRN1vP",
    "outputId": "dca858f0-112e-4a03-9b85-18176fc0e63a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# - Get the dataloader\n",
    "batch_size = 128\n",
    "trainloader, testloader = load_cifar10(\n",
    "    batch_size=batch_size, path=os.path.expanduser(\"~/Data\")\n",
    ")\n",
    "\n",
    "# - Define model, criterion, optimizer and scheduler.\n",
    "model = resnet32()\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfGUyS1BN1vQ",
    "outputId": "a3250fda-89f9-4f29-af1b-eca17170dba1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# - Get the dataloader\n",
    "batch_size = 128\n",
    "trainloader, testloader = load_cifar10(\n",
    "    batch_size=batch_size, path=os.path.expanduser(\"~/Data/\")\n",
    ")\n",
    "\n",
    "# - Change to True if one of the models should be re-trained\n",
    "retrain_baseline = False\n",
    "retrain_finetuned_model = False\n",
    "\n",
    "# - Some hyperparameters\n",
    "lr = 0.05\n",
    "epochs = 200\n",
    "epochs_finetuning = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuXtxjjrjEF0"
   },
   "source": [
    "Convert the ResNet model to its Analog version. Initialize the analog model withpre-trained and fine-tuned model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZ0mkeqXN1vQ",
    "outputId": "1ea1cc0a-d7ae-4afc-e076-7f7a3e8383b5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "analog_model = convert_to_analog(model, gen_rpu_config())\n",
    "!wget -P Models/ https://aihwkit-tutorial.s3.us-east.cloud-object-storage.appdomain.cloud/finetuned_model.th\n",
    "analog_model.load_state_dict(\n",
    "    torch.load(\"Models/finetuned_model.th\", map_location=device)\n",
    ")\n",
    "for tile_name, tile in analog_model.named_analog_tiles():\n",
    "    print(f\"{tile_name} input range {tile.input_range.data.item():.4f}\")\n",
    "print(f\"Test acc. (learned input ranges) {test_step(analog_model, criterion, testloader)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPERxGZ2N1vR"
   },
   "source": [
    "The calibration routine expects an iterator that feeds single elements that the network expects as inputs. These elements can be anything (e.g., tensors or dictionaries) as long as the model can process them.\n",
    "The next class demonstrates a simple `Sampler` that takes as input another iterator (torch dataloader) and yields only the images. The iterator can be halted by raising the `StopIteration` exception, which is used in this case to limit the number of presented batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxEJ1nV_N1vS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \"\"\"Example of a sampler used for calibration.\"\"\"\n",
    "\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.idx = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        x, _ = next(self.loader)\n",
    "        self.idx += 1\n",
    "        if self.idx > 10:\n",
    "            raise StopIteration\n",
    "        return x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2Qr76B5N1vT"
   },
   "source": [
    "We will now sweep the quantiles and look at the downstream performance for each quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9CIiij5XN1vT",
    "outputId": "db59a171-6a41-4309-b9a2-788d1d975535",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.0017 test acc. 94.34%\n",
      "Test loss 0.0017 test acc. 93.82%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "quantiles = [0.99995, 0.9995, 0.995, 0.95]\n",
    "for quantile in quantiles:\n",
    "    sampler = Sampler(trainloader)\n",
    "    calibrate_input_ranges(\n",
    "        model=analog_model,\n",
    "        calibration_type=InputRangeCalibrationType.CACHE_QUANTILE,\n",
    "        dataloader=sampler,\n",
    "        quantile=quantile,\n",
    "        max_samples=1000\n",
    "    )\n",
    "    test_acc = test_step(analog_model, criterion, testloader)\n",
    "    input_ranges = [tile.input_range.data.item() for tile in analog_model.analog_tiles()]\n",
    "    plt.plot(input_ranges, label=f\"Quantile {quantile:.5f} acc. {test_acc:.2f}\")\n",
    "\n",
    "plt.xlabel(\"Analog tile index\")\n",
    "plt.ylabel(\"Input range\")\n",
    "plt.legend()\n",
    "plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMBELmbgN1vT"
   },
   "source": [
    "As you can see from the above graph, the best quantile is 0.9995, as it yields the best accuracy. For more modes of calibrating the input ranges, check out `aihwkit.simulator.parameters.enums.InputRangeCalibrationType`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "8552d10987f98ef8320154686d1598142476aa8dfb1019e7b5164f51d5b1d29f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
